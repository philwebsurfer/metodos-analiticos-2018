# Similitud: Locality sensitive hashing

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
cb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


En esta parte continuaremos con la búsqueda de pares similares
para colecciones de textos, y después mostraremos cómo aplicar
estas técnicas para otras medidas de distancia  (como distancia
euclideana y coseno). 

Como vimos en la parte anterior, la técnica de LSH (locality sensitive
hashing) consiste en poner en cubetas a elementos que tengan 
hashes similares. Si diseñamos correctamente el método, entonces
no es necesario hacer todas las comparaciones entre los pares,
y basta examinar los elementos que compartan cubeta con otros elementos
(eliminando la mayor parte de las cubetas que tendrán solo un elemento).


## Análisis de la técnica de bandas 

En la sección anterior dimos la primera idea como usar
la *técnica de bandas* con minhashes para encontrar documentos de similitud alta, con distintos umbrales de similitud alta. Aquí describimos un análisis
más detallado de la técncia

```{block2 , type='resumen'}
Supongamos que tenemos un total de $k$ minhashes, que dividimos
en $b$ bandas de tamaño $r$, de modo que $k=br$. 

- Decimos que un par de documentos *coinciden* en una banda de $r$ hashes
si coinciden en todos los hashes de esa banda.

- Un par de documentos es un **par candidato** si 
por al menos coinciden en una banda (es decir, en al menos dentro
de una banda todos los hashes coinciden).

```

Ahora vamos a calcular la probabilidad de que un par de documentos
con similitud $s$ sean un par candidato:

1. La probabilidad de que estos dos documentos coincidan en un hash
particular es $s$, la similitud de Jaccard.
2. La probabiliad de que todos los hashes de una banda coincidan es
$s^r$, pues seleccionamos los hashes independientemente. 
3. Así que la probabilidad de que los documentos no coincidan en una banda
particular es:
es $1-s^r$
4. Esto implica que la probabilidad de que los documentos no coincidan en ninguna banda es $(1-s^r)^b$.
5. Finalmente, la probabilidad de que estos dos documentos sean un par candidato es $1-(1-s^r)^b$, que es la probabilidad de que coincidan en al menos una banda.

```{block2, type="resumen"}
Si la similitud de jaccard de dos documentos es $s$, la probabilidad
de que sean un par candidato es igual a $$1-(1-s^r)^b$$.

```



### Ejemplo {-}
Supongamos que tenemos 8 minhashes, y que nos
interesa encontrar documentos con similitud mayor a 0.7. 
Tenemos las siguientes posiblidades:

```{r, fig.width=4, fig.asp=0.6, echo = FALSE}
graficar_curvas <- function(df_br, colour = TRUE){
  r <- df_br$r
  b <- df_br$b
  datos_graf <- data_frame(s = seq(0, 1, 0.01))
  curvas_similitud <- data_frame(b = b, r =r) %>%
                    group_by(r, b) %>%
                    mutate(datos = map2(r, b, function(r, b){
                      datos_graf %>% 
                      mutate(prob = 1 - (1 - s ^ r) ^b)
                    })) %>%
                    unnest
  graf_salida <- ggplot(curvas_similitud, 
                        aes(x = s, y = prob, 
                            colour = as.factor(interaction(b,r)))) +
                 geom_line(size=1.1) + 
                 labs(x = 'similitud', y= 'probablidad de ser candidato',
                      colour = 'b.r') 
  if(colour){
    graf_salida + scale_colour_manual(values=cb_palette)
  }
                 
  graf_salida
}
```

```{r, fig.width=4, fig.asp=0.6}
r <- c(1,2,4,8)
df_br <- data_frame(r = r, b = rev(r))
graficar_curvas(df_br) + 
                 geom_vline(xintercept = 0.7)
```

- Con la configuración $b=1, r=8$ (un solo grupo de 8 hashes) es posible
que no capturemos muchos pares de la similitud que nos interesa.
- Con $b=8, r=1$ (al menos un hash de los 8), dejamos pasar 
demasiados falsos positivos, que después vamos a tener que filtrar.
- Los otros dos casos son mejores para nuestro propósito. $b=4$ produce falsos negativos que hay que filtrar, y para $b=2$ hay una probabilidad de alrededor de 50\%
de que no capturemos pares con similitud cercana a 0.7

Generalmente quisiéramos obtener algo más cercano a una función escalón.
Podemos acercarnos si incrementamos el número total de hashes.

```{r, fig.width=4, fig.asp=0.6}
r <- c(4, 5, 8, 10, 20)
b <- 80/r
graficar_curvas(data_frame(b, r)) +
                 geom_vline(xintercept = 0.7) 
```

---

**Observación**: La curva alcanza probabilidad 1/2 cuando la similitud
es
$$s = \left (1-\left (0.5\right )^{1/b} \right )^{1/r}.$$
Y podemos usar esta fórmula para escoger valores de $b$ y $r$ apropiados,
dependiendo de que similitud nos interesa capturar (quizá moviendo un poco
hacia abajo si queremos tener menos falsos negativos).
```{r}
lsh_half <- function(h, b){
   (1 - (0.5) ^ ( 1/b))^(b/h)
}
lsh_half(20,5)
```

En [@mmd], se utiliza la aproximación (del nivel de similitud
con máxima pendiente de la curva S, según la referencia):
```{r}
textreuse::lsh_threshold
```

Que está también implementada en el paquete textreuse [@R-textreuse].

```{r}
textreuse::lsh_threshold(20,5)
```
### Ejemplo {-}

Supongamos que nos interesan documentos con similitud mayor a 0.5.
Intentamos con 50 o 120 hashes algunas combinaciones:

```{r, fig.width=5, fig.asp=0.6}
params_umbral <- function(num_hashes, umbral_inf, umbral_sup){
  b <- seq(1, num_hashes)
  b <- b[ num_hashes %% b == 0]
  r <- num_hashes %/% b
  combinaciones_pr <- 
    data_frame(b = b, r = r) %>%
    unique() %>%
    mutate(s = (1 - (0.5)^(1/b))^(1/r)) %>%
    filter(s < umbral_sup, s > umbral_inf)
  combinaciones_pr
}

combinaciones_50 <- params_umbral(50, 0.0, 1.0)
graficar_curvas(combinaciones_50)
```

Con 120 hashes podemos obtener curvas con mayor pendiente:

```{r, fig.width=5, fig.asp=0.6}
combinaciones_120 <- params_umbral(120, 0.2, 0.6)
graficar_curvas(combinaciones_120)
```

**Observación**: La decisión de los valores para estos parámetros
debe balancear qué tan importante es tener pares no detectados,
y el cómputo necesario para calcular los hashes y filtrar los
falsos positivos. La ventaja computacional de LSH proviene
de hacer *trade-offs* de lo que es más importante para nuestro
problema.


## Resumen de LSH basado en minhashing

Resumen de [@mmd]

1. Escogemos un número $k$ de tamaño de tejas, y construimos el
conjunto de tejas de cada documento.
2. Ordenar los pares documento-teja y agrupar por teja.
3. Escoger $n$, el número de minhashes. Aplicamos el algoritmo de la
clase anterior (teja por teja) para calcular las 
firmas minhash de todos los documentos. 
4. Escoger el umbral $s$ de similitud que nos ineresa. Escogemos $b$ y $r$
(número de bandas y de qué tamaño), usando la fórmula de arriba hasta
obtener un valor cercano al umbral. 
Si es importante evitar falsos negativos, escoger valores de b y r que
den un umbral más bajo, si la velocidad es importante entonces escoger
para un umbral más alto y evitar falsos positivos. Mayores valores
de $b$ y $r$ pueden dar mejores resultados, pero también requieren
más cómputo.
5. Construir pares similares usando LSH
6. Examinar las firmas de cada par candidato y determinar si 
la fracción de coincidencias sobre todos los minhashes es satisfactorio.
Alternativamente (más preciso), calcular directamente la similitud 
de jaccard a partir de las tejas originales. 


Alternativamente, podemos:

2. Agrupar las tejas de cada documento
3. Escoger $n$, el número de minhashes. Calcular el minhash de cada
documento aplicando una función hash a las tejas del documento.
Tomar el mínimo. Repetir para cada función hash.

## Ejemplo: artículos de wikipedia

En este ejemplo intentamos encontrar artículos similares de [wikipedia](http://wiki.dbpedia.org/datasets/dbpedia-version-2016-10)
 usando las categorías a las que pertenecen. En lugar de usar tejas,
usaremos categorías a las que pertenecen. Dos artículos tienen similitud alta cuando los conjuntos de categorías a las que pertenecen es similar.
(este el [ejemplo original](https://github.com/elmer-garduno/metodos-analiticos/blob/master/Lecture_2_Similarity_Spark.ipynb)).


```{r, engine='bash'}
head -20 ../datos/similitud/wiki-100000.txt
```


Primero hacemos una versión en memoria  usando *textreuse*

```{r, message = FALSE}
library(textreuse)
limpiar <- function(lineas,...){
  df_lista <- str_split(lineas, ' ') %>% 
    keep(function(x) x[1] != '#') %>%
    transpose %>%
    map(function(col) as.character(col)) 
  df <- data_frame(articulo = df_lista[[1]], 
                   categorias = df_lista[[2]]) 
  df
}
filtrado <- read_lines_chunked('../datos/similitud/wiki-100000.txt',
                    skip = 1, callback = ListCallback$new(limpiar))
articulos_df <- filtrado %>% bind_rows %>%
                group_by(articulo) %>%
                summarise(categorias = list(categorias))
```

```{r}
set.seed(99)
muestra <- articulos_df %>% sample_n(10)
muestra
muestra$categorias[[10]]
```

### Selección de número de hashes y bandas {-}

Ahora supongamos que buscamos artículos con similitud mínima
de 0.4. Experimentando con valores del total de hashes y el número
de bandas, podemos seleccionar, por ejemplo:

```{r, collapse = TRUE, fig.width=5, fig.asp=0.6}
b <- 20
num_hashes <- 60
lsh_half(num_hashes, b = b)
graficar_curvas(data_frame(b = b, r = num_hashes/b)) +
                 geom_vline(xintercept = 0.4) 
```



### Tejas y cálculo de minhashes {-}

```{r, echo=FALSE}
shingle_chars <- function(string, lowercase = FALSE, k = 3){
    # produce shingles (con repeticiones)
    
    if(lowercase) {
      string <- str_to_lower(string)
    }
    shingles <- seq(1, nchar(string) - k + 1) %>%
        map_chr(function(x) substr(string, x, x + k - 1))
    shingles
}
```



```{r}
options("mc.cores" = 4L)
# esta es la función que vamos a usar:
tokenize_sp <- function(x) str_split(x, ' ', simplify = TRUE)
# aunque otra opción es:
minhashes <- minhash_generator(num_hashes, seed = 1223)
# esta línea solo es necesaria porque TextReuseCorpus espera una
# línea de texto, no un vector de tokens.
textos <- articulos_df$categorias %>% 
          lapply(function(x) paste(x, collapse = ' ')) %>%
          as.character
names(textos) <- articulos_df$articulo
system.time(
wiki_corpus <-  TextReuseCorpus(
                text = textos, 
                tokenizer = tokenize_sp,
                minhash_func = minhashes,
                skip_short = FALSE)
)
```

```{r}
str(wiki_corpus[[1002]])
```

### Agrupar en cubetas {-}


```{r}
lsh_wiki <- lsh(wiki_corpus, bands = 20)
```

```{r}
lsh_wiki %>% sample_n(20)
```

**Observación**: en la parte anterior sugerimos que podíamos
normalizar los nombres de las cubetas. Esto también lo podemos
hacer con funciones hash. Podemos usar por ejemplo el algoritmo *md5*,
que está implementado para hacer hashes de objetos de R arbitrarios.

```{r}
library(digest)
digest(c(-2341, 2221 , 21112))
digest(c('una' , 'dos'))
x <- c(0,1); y <- c(2,3)
a <- lm(y~x)
digest(a)
```


Agrupamos por cubetas y filtramos las cubetas con más de un documento:

```{r}
cubetas_df <- lsh_wiki %>% 
             group_by(buckets) %>%
             summarise(candidatos = list(doc)) %>%
             mutate(num_docs = map_int(candidatos, length)) %>%
             filter(num_docs > 1)
```

```{r}
cubetas_df <- cubetas_df %>% arrange(desc(num_docs)) 
nrow(cubetas_df)
```

```{r}
sample_n(cubetas_df, 20)
```

```{r}
cubetas_df$candidatos[[1]]
lapply(cubetas_df$candidatos[[1]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

```{r}
cubetas_df$candidatos[[714]]
lapply(cubetas_df$candidatos[[714]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

```{r}
cubetas_df$candidatos[[911]]
lapply(cubetas_df$candidatos[[911]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```
---

## Consulta de pares candidatos.

Si tenemos un documento dado (nuevo o de la colección) 
y queremos encontrar candidatos
similares, podemos hacerlo usando la estructura de
LSH que acabamos de construir, sin tener que
recorrer todos los documentos.  Podemos hacer:

- Construimos la firma minhash del documento.
- Calculamos las cubetas donde este documento cae, y buscamos
estas cubetas que creamos para la colección
- Extraemos los elementos que están en estas cubetas.



### Ejempo: Consulta de pares candidatos {-}

Podemos buscar más fácilmente candidatos similares:

```{r}
lsh_query(lsh_wiki, 'October_1')
```

```{r}
lsh_query(lsh_wiki, 'Icosahedron')
```

Veamos por qué esta última lista se ve así. Examinamos dos ejemplos, y
vemos que en efecto su similitud no es tan baja:

```{r}
wiki_corpus[["Icosahedron"]]
wiki_corpus[["Disaster"]]

```

```{r}
minhash_estimate <- function(a, b, corpus){
  mean(corpus[[a]]$minhashes == corpus[[b]]$minhashes)
}

lsh_query(lsh_wiki, 'Icosahedron') %>% 
  rowwise %>%
  mutate(score = minhash_estimate(a, b, wiki_corpus)) %>% 
  arrange(desc(score))

```



### Filtrar falsos positivos {-}

Y también podemos preprocesar todos los candidatos y eliminar
los falsos positivos:


```{r}
wiki_candidatos <- lsh_candidates(lsh_wiki)
wiki_candidatos %>% nrow
```

Tenemos que evaluar estos resultados (antes tendríamos
que haber evaluado alrededor de 127 millones de pares). Calculamos
el score:

```{r}
wiki_candidatos <- 
  wiki_candidatos %>% 
  rowwise %>%
  mutate(score = minhash_estimate(a, b, wiki_corpus))
wiki_candidatos %>% sample_n(20)
```

```{r}
qplot(wiki_candidatos$score)
candidatos_finales <- filter(wiki_candidatos, score > 0.4)
nrow(candidatos_finales)
```

```{r}
candidatos_finales %>% 
  sample_n(200)
```

```{r, message = FALSE}
lsh_query(lsh_wiki, 'Economy_of_Paraguay') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```


```{r, message = FALSE}
lsh_query(lsh_wiki, 'Icosahedron') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```

```{r, message = FALSE}
lsh_query(lsh_wiki, 'Ghana') %>% 
  left_join(candidatos_finales) %>%
  filter(!is.na(score))
```


## Candidatos idénticos

Cuando buscamos candidatos idénticos, podemo intentar
otras estrategias. Si los documentos no son muy grandes, podemos
hacer hash del documento entero a un número grande de cubetas. Si el
número de cubetas es suficientemente grande para la colección
de texto, entonces cualquier par de documentos que caigan en una 
misma cubeta serán idénticos con muy alta probabilidad.

Para documentos más grandes, podemos tomar, por ejemplo, una selección
al azar de posiciones en el documento y usar funciones hash. Puedes
revisar otras técnicas en [@mmd], Sección 3.9

### Ejemplo {-}

En el caso de artículos de wikipedia vimos algunas cubetas que
artículos que contenían exactamente las mismas categorías. Podríamos
hacer, por ejemplo:

```{r}
articulos_df
hash_categorias <- articulos_df %>% 
                   mutate(hash_doc = map_chr(categorias, 
                                    function(x) digest(sort(x)))) 

hash_categorias %>% filter(articulo %in% c('April_1','April_10'))

hash_categorias <- hash_categorias %>%
                   select(hash_doc, articulo) %>%
                   group_by(hash_doc) %>%
                   summarise(articulo = list(articulo)) %>%
                   mutate(num_docs = map_int(articulo, length)) %>%
                   filter(num_docs > 1) %>%
                   arrange(desc(num_docs))
hash_categorias
```

De esta tabla podemos obtener los pares idénticos:

```{r}
hash_categorias$articulo[[2]]
articulos_df %>% filter(articulo == 'Abingdon') %>% pull(categorias)
```

Y podemos eliminar de nuestros candidatos, para reducir el trabajo 
de cálculo:

```{r}
df_1 <- data_frame(a = unlist(hash_categorias$articulo))
wiki_candidatos_noid <- wiki_candidatos %>%
                        anti_join(df_1) %>%
                        anti_join(df_1 %>% rename(b=a)) 
qplot(wiki_candidatos_noid$score)
```

## Medidas de distancia

La técnica de LSH puede aplicarse a otras medidas de distancia, con
otras formas de hacer hash diferente del minhash. La definición
de distancia puedes consultarla [aquí](https://en.wikipedia.org/wiki/Metric_(mathematics))

### Distancia de Jaccard

Puede definirse simplemente como 
$$1-sim(a,b),$$
donde $a$ y $b$ son conjuntos y $sim$ es la similitud de Jaccard.

### Distancia euclideana

Es la distancia más común para vectores de números reales:

Si $x=(x_1,\ldots, x_p)$ y $y=(y_1,\ldots, y_p)$ son dos vectores,
su norma $L_2$ está dada por

$$ d(x,y) = \sqrt{\sum_{i=1}^p (x_i-y_i)^2  } = ||x-y||$$

### Distancia coseno

La distancia coseno, definida también para vectores de números reales, no toma en cuenta la magnitud de vectores, sino solamente su dirección.

La similitud coseno se define primero como
$$sim_{cos}(x,y) = \frac{<x, y>}{||x||||y||} = \cos (\theta)$$
donde $<x, y> = \sum_{i=1}^p x_iy_i$ es el producto punto de $x$ y $y$. Esta cantidad es igual al coseno del ángulo entre los vectores $x$ y $y$ (¿por qué?).


La
distancia coseno es entones
$$d_{cos}(x,y) = 1- sim_{cos}(x,y).$$

Esta distancia es útil cuando el tamaño general de los vectores no nos importa. Como veremos más adelante, una aplicación usual es comparar
documentos según las frecuencias de los términos que contienen: en este
caso, nos importa más la frecuencia relativa de los términos que su frecuencia absoluta (pues esta última también refleja la el tamaño de los documentos).

A veces se utiliza la distancia angular (medida con un número entre 0 y 180), que se obtiene de la distancia coseno, es decir,
$$d_a(x,y) = \theta,$$
donde $\theta$ es tal que $\cos(\theta) = d_{cos}(x,y).$

### Distancia de edición

Esta es una medida útil para medir distancia entre cadena. La
distancia de edición entre dos cadenas $x=x_1\cdots x_n$ y 
$y=y_1\cdots y_n$ es el número mínimo de inserciones y eliminaciones (un caracter a la vez) para convertir a $x$ en $y$. 

Por ejemplo, la distancia entre "abcde" y "cefgh" se calcula como
sigue: para pasar de la primera cadena, necesitamos agregar f, g y h (3 adiciones), eliminar d, y eliminar a,b (3 eliminaciones). La distancia entre estas dos cadenas es 6.


## Teoría de funciones sensibles a la localidad


Vimos como la familia de funciones minhash puede combinarse (usando 
la técnica de bandas) para discriminar entre pares de baja similitud
y de alta similitud.

En esta parte consideramos otras posibles familias de funciones para lograr 
lo mismo (hacer LSH), bajo otras medidas de distancia. Veamos las características básicas de las funciones minhash:

 1. Cuando la distancia entre dos elementos $x,y$ es baja (similitud alta),
 entonces $f(x)=f(y)$ tiene probabilidad alta.
 2. Podemos escoger varias funciones  $f_1,\ldots,f_k$ con la propiedad anterior, de manera independiente, de forma que es posible calcular
 la probabilidad de $f_1(x)=f_1(y)$ y $f_2(x)=f_2(y)$, y eventos
 combinados de este tipo.
 3. Las funciones tienen que ser relativamente fáciles de calcular (comparado con calcular todos los posibles pares y sus distancias directamente).
 
 
## Funciones sensibles a la localidad


```{block2, type="resumen"}
Sean $d_1<d_2$ dos valores (que interpretamos como distancias).

Una familia ${\cal F}$ es una familia $d_1,d_2,p_1,p_2$,  sensible a localidad
(con $p_1>p_2$) cuando para cualquier par de elementos$x,y$,

1. Si $d(x,y)\leq d_1$, entonces la probabilidad  $P(f(x)=f(y))\geq p_1$.
2. Si $d(x,y)\geq d_2$, entonces $P(f(x)=f(y))\leq p_2$

Nótese que las probabilidades están dadas sobre la selección de $f$.
```
  


Estas condiciones se interpretan como sigue: cuando $x$ y $y$ están
suficientemente cerca ($d_1$), la probabilidad de que sean mapeados al mismo valor
por una función $f$ de la familia es alta.  Cuando $x$ y $y$ están lejos
$d_2$, entonces, la probabilidad de que sean mapeados al mismo valor es baja.
Podemos ver una gráfica:   


```{r}
x_1 <- seq(0, 1, 0.01)
x_2 <- seq(2, 3, 0.01)
y_1 <- -1*x_1 + 2.5
y_2 <- 2/x_2

dat_g <- data_frame(x=c(x_1,x_2),y=c(y_1,y_2))
ggplot(dat_g, aes(x=x, y=y)) + geom_point(size=0.5) +
  geom_vline(xintercept=c(1,2), linetype="dotted") +
  geom_hline(yintercept=c(1,1.5), linetype="dotted") +
  scale_x_continuous(breaks = c(1,2), labels = c('d_1','d_2')) +
  scale_y_continuous(breaks = c(1,1.5), labels = c('p_2','p_1')) +
  labs(x = 'Distancia', y ='Probabilidad de candidato')
```

### Distancia jaccard

Supongamos que tenemos dos documentos $x,y$. Si ponemos por ejemplo
$d_1=0.2$ y $d_2= 0.5$, tenemos que 
si $d(x,y) = 1-sim(x,y) \leq 0.2$, despejando tenemos
$sim(x,y)\geq 0.8$, y entonces
$$P(f(x) = f(y)) = sim(x,y) \geq 0.8$$
Igualmente, si $d(x,y)=1-sim(x,y) \geq 0.5$, entonces
$$P(f(x) = f(y)) = sim(x,y) \leq 0.5$$
de modo que la familia de minhashes es $(0.2,0.5,0.8,0.5)$ sensible a la
localidad

```{block2, type="resumen"}
Para cualquier $d_1 < d_2$,
la familia de funciones minhash es una familia 
$(d_1, d_2, 1-d_1, 1-d_2)$ sensible a la localidad para cualquier
$d_1\leq d_2$.
```

## Amplificación de familias sensibles a la localidad

Con una familia sensible a la localidad es posible usar la técnica
de bandas para obtener la discriminación de similitud que nos interese.

Supongamos que ${\cal F}$ es una familia $(d_1, d_2, p_1, p_2)$-sensible
a la localidad. Podemos usar **conjunción** de ${\cal F}'$ para construir
otra familia sensible a la localidad.

Sea $r$ un número entero. Una función $f\in {\cal F}'$ se construye
tomando $f = (f_1,f_2,\ldots, f_r)$, con $f_i$ seleccionadas al
azar de manera independiente de la familia original, de forma
que $f(x)=f(y)$ si y sólo si $f_i(x)=f_i(y)$ para toda $i$. Esta construcción
corresponde a lo que sucede dentro de una banda de la técnica de LSH.

La nueva familia ${\cal F}'$ es $(d_1,d_2,p_1^r,p_2^r)$ sensible a la localidad. Nótese que las probabilidades siempre se hacen más chicas
cuando incrementamos $r$, lo que hace más fácil discriminar pares
con similitudes en niveles bajos.


Podemos también hacer **disyunción** de una familia  ${\cal F}$. En este
caso, decimos que $f(x)=f(y)$ cuando al menos algún
$f_i(x)=f_i(y)$.

En este caso, la disyunción da una familia
$(d_1,d_2,1-(1-p_1)^b,1-(1-p_2)^b)$ sensible a la localidad. Esta construcción
es equivalente a construir varias bandas.

La idea general es ahora:

```{block2, type="resumen"}
- Usando **conjunción**, podemos construir una familia donde
la probabilidad $p_2^r$ sea mucho más cercana a cero que
$p_1^r$ (en términos relativos). 

- Usando **disyunción**, podemos construir una familia donde
la probabilidad $1-(1-p_1^r)^b$ permanece cercana a 1,
pero $1-(1-p_2^r)^b$ está cerca de cero.

- Combinando estas operaciones usando la técnica de bandas
podemos construir una famlia que discrimine de manera distinta
entre distancias menores a $d_1$ y distancias mayores a $d_2$.

- El costo incurrido es que tenemos que calcular más funciones para
discriminar mejor.
```

### Ejercicio {-}
Supongamos que tenemos una familia $(0.2, 0.6, 0.8, 0.4)$ sensible
a la localidad. Si combinamos con conjunción 4 de estas funciones,
obtenemos una familia
$$(0.2, 0.6, 0.41, 0.026)$$
La proporción de falsos positivos es chica, pero la de falsos negativos
es grande. Si tomamos 8 de estas funciones (cada una compuesta de
cuatro funciones de la familia original) y hacemos conjunción, obtenemos una familia

$$(0.2, 0.6, 0.98, 0.19)$$

En esta nueva familia, tenemos que hacer 40 veces más trabajo para
tener esta amplificación.

---

## Distancia coseno e hiperplanos aleatorios

Construimos ahora LSH para datos numéricos, y comenzaremos
con la distancia coseno. Lo primero que necesitamos es
una familia sensible a la localidad para la distancia coseno.

Consideremos dos vectores, y supongamos que el ángulo entre ellos
es chico. Si escogemos un hiperplano al azar, lo más
probable es que queden del mismo lado del hiperplano. En el caso extremo, 
si los vectores
apuntan exactamente en la misma dirección, entonces la probabilidad es 1.

Sin embargo, si el ángulo entre estos vectores es grande, entonces lo más probable es que queden separados por un hiperplano escogido al azar. Si los vectores son ortogonales (máxima distancia coseno posible), entonces
esta probabilidad es 0.

Esto sugiere construir una familia sensible a la localidad para
la distancia coseno de la siguiente forma:

- Tomamos un vector al azar $v$.
- Nos fijamos en la componente de la proyección de $x$ sobre $v$ 
- Ponemos $f_v(x)=1$ si esta componente es positiva, y
$f_v(x)=-1$ si esta componente es negativa.
- Podemos poner simplemente:
$$ f_v(x) = signo (<x, v>)$$

**Recordatorio**: La componente de la proyección de $x$ sobre $v$ está
dada por el producto interior de $x$ y $v$ normalizado:
$$\frac{1}{||v||}<x, v>,$$
y su signo es el mismo de $<x,v>$.


```{block2, type="resumen"}
La familia descrita arriba (hiperplanos aleatorios) 
es $(d_1,d_2, (180-d_1)/180, d_2/180)$
  sensible a la localidad para la distancia angular.
```

Vamos a dar un argumento del cálculo: supongamos que el ángulo entre $x$ y $y$ es $d=\theta$, es decir,
la distancia angular entre $x$ y $y$ es $\theta$.  

Consideramos el plano $P$ que pasa por el origen por $x$ y $y$. 
Si escogemos un vector al azar (cualquier dirección igualmente probable), el vector produce un hiperplano perpendicular (son
los puntos $z$ talqes que $<z,v>=0$)
que corta al plano $P$
en dos partes.
Todas las direcciones de corte son igualmente probables, así
que la probabilidad de que la dirección de corte separe a $x$ y $y$
es igual a $2\theta /360$ (que caiga en el cono generado por $x$ y $y$).
Si la dirección de corte separa a $x$ y $y$, entonces sus valores
$f_v(x)$ y $f_v(y)$ no coinciden, y coinciden si la dirección
no separa a $x$ y $y$. Así que:

1. $d(x,y)=d_1=\theta$, entonces $P(f(x)=f(y)) = d_1/180.$

Por otra lado, 

2. $d(x,y)=d_2$, entonces  $P(f(x)\neq f(y)) = 1-d_2/180.$

---

### Ejemplo: similitud coseno por fuerza bruta {-}

Comenzamos con un ejemplo simulado.

```{r}
set.seed(101)
mat_1 <- matrix(rnorm(300 * 1000) + 3, ncol = 1000)
mat_2 <- matrix(rnorm(600 * 1000) + 0.2, ncol = 1000)
df <- rbind(mat_1, mat_2) %>% data.frame %>%
           add_column(id_1 = 1:900, .before = 1)
head(df[,1:5])
```

Tenemos entonces 1000 variables distintas y 900 casos, y nos
interesa filtrar aquellos pares de similitud alta.

Definimos nuestra función de distancia

```{r}
norma <- function(x){
  sqrt(sum(x ^ 2))
}
dist_coseno <- function(x, y){
  1 - sum(x*y) / (norma(x) * norma(y))
}
```


Y calculamos todas las posibles distancias (normalmente
 **no** queremos hacer esto, pero lo hacemos aquí para
 comparar):

```{r}
df_agrup <- df %>% gather('variable', 'valor', -id_1) %>%
                   group_by(id_1) %>%
                   arrange(variable) %>%
                   summarise(vec_1 = list(valor))
df_pares <- df_agrup %>% 
            crossing(df_agrup %>% 
                       rename(id_2 = id_1, vec_2 = vec_1)) %>%
            filter(id_1 < id_2) %>%
            mutate(dist = map2_dbl(vec_1, vec_2, dist_coseno))
df_pares
```

La distribución de distancias sobre todos los pares es la siguiente:
(¿por qué observamos este patrón? Recuerda que esta gráfica
 representa pares):

```{r, fig.width=5, fig.asp = 0.8}
qplot(df_pares$dist, binwidth = 0.01)
```

Y supongamos que queremos encontrar vectores con distancia
coseno menor a 0.2 (menos de unos 40 grados). El número de pares que satisfacen
esta condicion son:


```{r}
sum(df_pares$dist < 0.20)
```

### Ejemplo: LSH planos aleatorios {-}

Con 200 funciones hash:

```{r}
set.seed(101021)
hashes <- lapply(1:200, function(i){
    v <- rnorm(1000)
    function(x){
        ifelse(sum(v*x) >= 0, 1, -1) 
    }
})
```

Por ejemplo, la firma del primer elemento es:

```{r}
x <- as.numeric(df[1,-1])
sapply(hashes, function(f) f(x))
```


Y ahora calcuamos la firma para cada elemento:

```{r}
df_hash <- df_agrup %>%
           mutate(df = map(vec_1, function(x){
              firma <-  sapply(hashes, function(f) f(x)) 
              data_frame(id_hash = 1:length(firma),
                         firma = firma) })) %>% 
              select(-vec_1) %>% unnest
df_hash
```


Vamos a amplificar la famiia de hashes. En este caso,
escogemos 20 bandas de 10 bandas cada una.

```{r, fig.width=5, fig.asp=0.8}
f_1 <- function(x){
    1-(1-((180-x)/180)^10)^20
}
curve(f_1, 0, 180)
abline(v=20)
```


### Ejemplo: agrupar por cubetas para LSH {-}

Ahora agrupamos y construimos las cubetas:

```{r}
df_hash_1 <- df_hash %>% 
           mutate(banda  = (id_hash - 1) %% 20 + 1) %>%
           mutate(h = paste(id_hash,firma)) %>%
           arrange(id_1)
df_hash_1
```


```{r}
cubetas <- df_hash_1 %>% 
             group_by(id_1, banda) %>%
             summarise(cubeta = paste(h, collapse = '/')) 
cubetas
```

```{r}
cubetas_hash <- cubetas %>%
                ungroup %>% rowwise %>%
                mutate(cubeta = digest::digest(cubeta))
cubetas_hash
```


```{r}
cubetas_agrup <- cubetas_hash %>% group_by(cubeta) %>%
                  summarise(ids = list(id_1)) %>%
                  mutate(num_ids = map_dbl(ids, length)) %>%
                  filter(num_ids > 1 )
cubetas_agrup
```

Y ahora extraemos los pares similares

```{r}
pares_candidatos <- lapply(cubetas_agrup$ids, function(x){
  combn(sort(x), 2, simplify = FALSE)}) %>% 
  flatten %>% unique %>% 
  transpose %>% lapply(as.integer) %>% as.data.frame
names(pares_candidatos) <- c('id_1','id_2')
head(pares_candidatos)
```


### Ejemplo: filtrar y evaluar resultados {-}

Y ahora evaluamos nuestros resultados. En primer lugar, el 
número de pares reales y de candidatos es

```{r}
pares_reales <- filter(df_pares, dist < 0.15) %>%
                select(id_1, id_2)
nrow(pares_reales)
nrow(pares_candidatos)
```

Así que debemos tener buen número de falsos positivos. Podemos calcularlos haciendo
```{r}
nrow(anti_join(pares_candidatos, pares_reales))
```

Y el número de falsos negativos es

```{r}
nrow(anti_join(pares_reales, pares_candidatos))
```

que es un porcentaje bajo del total de pares reales.


---


**Observación*: es posible, en lugar de usar vectores con dirección
aleatoria $v$ escogidos al azar como arriba (con la distribución normal), hacer ménos cálculos escogiendo vectores $v$
cuyas entradas son solamente 1 y -1. El cálculo del producto
punto es simplemente multiplicar por menos si es necesario los
valores de los vectores $x$ y sumar.

## LSH para distancia euclideana.

Para distancia euclideana usamos el enfoque de proyecciones
aleatorias en cubetas.

La idea general es que tomamos una línea al azar en el espacio
de entradas, y la dividimos en cubetas de manera uniforme. El valor
hash de un punto $x$ es el número de cubeta donde cae la proyección de $x$.

```{block2, type='resumen'}
Supogamos que tomamos como $a$ el ancho de las cubetas.
La familia de proyecciones aleatorios por cubetas es
una familia
$(a/2, 2a, 1/2, 1/3)$-sensible a la localidad para la distancia 
euclideana.
```

Supongamos que dos punto $x$ y $y$ tienen distancia euclideana
$d = a/2$. Si proyectamos perpendicularmente sobre la línea
escogida al azar, la distancia entre las proyecciones es menor
a $a/2$, de modo la probabilidad de que caigan en la misma
cubeta es al menos 1/2. Si la distancia es menor, entonces la probabilidad
es más grande aún:

1. Si $d(x,y)\leq a/2$ entonces $P(f(x)=f(y))\geq 1/2$.

Por otro lado, si la distancia es mayor a $2a$, entonces la única
manera de que los dos puntos caigan en una misma cubeta es
que la distancia de sus proyecciones sea menor a $a$. Esto sólo
puede pasar si el ángulo entre el vector que va de $x$ a $y$ y
la línea escogida al azar es mayor de  60 a 90 grados. Como
$\frac{90-60}{90-0} = 1/3$, entonces la probabilidad que que
caigan en la misma cubeta no puede ser más de 1/3.

1. Si $d(x,y)\geq 2a$ entonces $P(f(x)=f(y))\leq 1/3$.

Escoger $a$ para discriminar las distancias que nos interesa,
y luego amplificar la familia para obtener tasas de falsos
positivos y negativos que sean aceptables.


## Tarea {-}

1. La tarea está en *scripts/lsh/entity-matching.Rmd*

2. Si quieres hacer experimentos con el error en el cálculo de similitud
por minhashing (y la diferencia entre permutaciones y hashes), puedes
ver el script *scripts/lsh/notas_adicionales_minhash.nb.html* (o correr el Rmd
asociado).


