{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen 2\n",
    "\n",
    "_ 175904 - Jorge III Altamirano Astorga_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------------+\n",
      "|   id|cuisine|         ingredients|\n",
      "+-----+-------+--------------------+\n",
      "|10259|  greek|romaine lettuce, ...|\n",
      "+-----+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import time, os, re, glob, sys\n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.worker.cleanup.appDataTtl\", 24*60*60)\n",
    "conf.set(\"spark.worker.cleanup.enabled\", True)\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.driver.cores\", 5)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.cores\", 5)\n",
    "conf.set(\"spark.jars\", \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.3.jar,\" +\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/config-1.3.0.jar,\" + #needed nlp\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar,\" + #needed by aws\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/commons-cli-1.2.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar\") #needed by aws\n",
    "conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.3\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "### get they creds to login to AWS :-)\n",
    "HOME = os.environ[\"HOME\"]\n",
    "aws_id, aws_key = (None, None)\n",
    "with open(HOME+\"/.aws/credentials\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"aws_access_key_id\" in line:\n",
    "            aws_id = re.sub(\"^.*aws_access_key_id\\s*=\\s*\", \"\", line)\n",
    "        elif \"aws_secret_access_key\" in line:\n",
    "            aws_key = re.sub(\"^.*aws_secret_access_key\\s*=\\s*\", \"\", line)\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_id)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_key)\n",
    "aws_id, aws_key = (None, None)\n",
    "### end getting keys\n",
    "sc = SparkContext(master = \"spark://jupyter.corp.penoles.mx:7077\", \n",
    "                  sparkHome=\"/usr/local/spark/\",\n",
    "                  appName=\"examen-ma-2\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "## setup sparknlp source\n",
    "## \n",
    "## https://github.com/JohnSnowLabs/spark-nlp/issues/106\n",
    "## https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error\n",
    "sys.path.extend(glob.glob(\"/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.3.jar\"))\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import pyspark.sql.functions as sparkFunctions\n",
    "\n",
    "train2 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\").cache()\n",
    "train2.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pipeline Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42\n",
    "# and https://stackoverflow.com/a/32337101/7323086\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.pipeline import Transformer \n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol \n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, IDF, HashingTF, IDF, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Create a custom word count transformer class \n",
    "class StringArray2PlainString(Transformer, HasInputCol, HasOutputCol): \n",
    "    @keyword_only \n",
    "    def __init__(self, inputCol=None, outputCol=None): \n",
    "        super(StringArray2PlainString, self).__init__() \n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs) \n",
    "    @keyword_only \n",
    "    def setParams(self, inputCol=None, outputCol=None): \n",
    "        kwargs = self._input_kwargs \n",
    "        return self._set(**kwargs) \n",
    "    def _transform(self, dataset): \n",
    "        out_col = self.getOutputCol() \n",
    "#         in_col = dataset[self.getInputCol()]\n",
    "        in_col = self.getInputCol()\n",
    "        finishr = Finisher() \\\n",
    "            .setInputCols([in_col]) \\\n",
    "            .setOutputCols([\"tmp_col\"]) \\\n",
    "            .setIncludeKeys(False) \\\n",
    "            .setAnnotationSplitSymbol(\" \")\n",
    "        tmp_ds = finishr.transform(dataset).cache()\n",
    "        return tmp_ds.withColumn(out_col, sparkFunctions.split(tmp_ds.tmp_col, \",\")).drop(\"tmp_col\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+--------------------+-----+\n",
      "|   id|    cuisine|         ingredients|        ingredients3|label|\n",
      "+-----+-----------+--------------------+--------------------+-----+\n",
      "|10259|      greek|romaine lettuce, ...|[romain lettuc , ...|  9.0|\n",
      "|25693|southern_us|plain flour, grou...|[plain flour ,  g...|  2.0|\n",
      "+-----+-----------+--------------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 187 ms, sys: 33.7 ms, total: 221 ms\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docAssemblr = DocumentAssembler()\\\n",
    "  .setInputCol(\"ingredients\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizr = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")#    .addInfixPattern(\"(\\p{L}+)(n't\\b)\") \\\n",
    "    \n",
    "normalizr = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setPattern(\"[^A-Za-z,]\")\n",
    "    \n",
    "# path_dict = \"file:/home/jaa6766/enwiki-latest-all-titles-in-ns0-transform\"\n",
    "path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/wiki-titles.txt\"\n",
    "norvig = NorvigSweetingApproach() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"ingredients2\") \\\n",
    "    .setDictionary(path_dict)\n",
    "# norvig.setCorpus(\"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\")\n",
    "    \n",
    "stemmr2 = Stemmer() \\\n",
    "  .setInputCols([\"ingredients2\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "\n",
    "ar2str = StringArray2PlainString(\n",
    "    inputCol=\"stems\", \n",
    "    outputCol=\"ingredients3\")\n",
    "\n",
    "word2v = Word2Vec() \\\n",
    "    .setVectorSize(40) \\\n",
    "    .setInputCol(\"ingredients3\") \\\n",
    "    .setOutputCol(\"word2vec\")\n",
    "    \n",
    "idf0 = IDF() \\\n",
    "    .setInputCol(\"ingredients3\") \\\n",
    "    .setOutputCol(\"idf\")\n",
    "\n",
    "si0 = StringIndexer(inputCol=\"cuisine\", outputCol=\"label\")\n",
    "\n",
    "va0 = VectorAssembler(inputCols=[\"idf\"], outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    normalizr,\n",
    "    norvig,\n",
    "    stemmr2,\n",
    "    ar2str,\n",
    "#     word2v,\n",
    "#     idf0,\n",
    "    si0,\n",
    "#     va0\n",
    "])\n",
    "model_data = pipeline.fit(train2)\n",
    "train4 = model_data.transform(train2).cache()\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayString2String(Transformer, HasInputCol, HasOutputCol): \n",
    "    @keyword_only \n",
    "    def __init__(self, inputCol=None, outputCol=None): \n",
    "        super(ArrayString2String, self).__init__() \n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs) \n",
    "    @keyword_only \n",
    "    def setParams(self, inputCol=None, outputCol=None): \n",
    "        kwargs = self._input_kwargs \n",
    "        return self._set(**kwargs) \n",
    "    def _transform(self, dataset): \n",
    "        out_col = self.getOutputCol() \n",
    "        in_col = self.getInputCol()\n",
    "        return (dataset\n",
    "                .withColumn(\"tmp_col0\", dataset[in_col].cast(StringType()))\n",
    "                .withColumn(\"tmp_col1\", regexp_replace(\"tmp_col0\", \"[\\[\\]]\", \"\"))\n",
    "                .withColumn(\"tmp_col2\", regexp_replace(\"tmp_col1\", \"\\s+\", \" \"))\n",
    "                .withColumn(out_col, regexp_replace(\"tmp_col2\", \"\\s*,\\s*\", \",\"))\n",
    "                .drop(\"tmp_col0\", \"tmp_col1\", \"tmp_col2\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "## IDF!!!\n",
    "ars2s = ArrayString2String(inputCol=\"ingredients3\", outputCol=\"ingredients4\")\n",
    "tknzr = RegexTokenizer(inputCol=\"ingredients4\", outputCol=\"ingredients5\", pattern=\",\")\n",
    "hshng = HashingTF(inputCol=\"ingredients5\", outputCol=\"ingredients6\")\n",
    "idf0  = IDF(inputCol=\"ingredients6\", outputCol=\"features\")\n",
    "pipe_hash = Pipeline(stages = [ars2s, tknzr, hshng, idf0])\n",
    "pipe_hashm = pipe_hash.fit(train4.cache())\n",
    "train5 = (pipe_hashm\n",
    " .transform(train4)\n",
    " .drop(\"ingredients\", \"ingredients3\", \"ingredients4\", \"ingredients5\", \"ingredients6\")\n",
    " .cache())\n",
    "#  .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 41.2 ms, total: 185 ms\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##############\n",
    "## IDF!!!\n",
    "(train_data, test_data) = train5.randomSplit([0.7, 0.3], seed = 175904)\n",
    "ml0 = LogisticRegression(predictionCol=\"preds\")\n",
    "is0 = IndexToString(inputCol=\"preds\", \n",
    "                    labels=model_data.stages[6].labels, \n",
    "                    outputCol=\"y_hat\")\n",
    "pipeml = Pipeline(stages=[ml0, is0])\n",
    "model_ml = pipeml.fit(train_data)\n",
    "test_data = model_ml.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para Logistic Regression: 0.680478 (measure f1)\n"
     ]
    }
   ],
   "source": [
    "ev0 = MulticlassClassificationEvaluator()\n",
    "ev0.setLabelCol(\"label\")\n",
    "ev0.setPredictionCol(\"preds\")\n",
    "score0 = ev0.evaluate(test_data)\n",
    "print(\"Score para Logistic Regression: %f (measure %s)\"%(\n",
    "    score0, ev0.getMetricName()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------------+------------+\n",
      "|   id|     cuisine|       y_hat|    y_hatRFC|\n",
      "+-----+------------+------------+------------+\n",
      "|   10|     chinese|     chinese|     chinese|\n",
      "| 1000|     mexican|     mexican|     mexican|\n",
      "|10003|      french|     spanish|     italian|\n",
      "|10006|     italian|     italian|     italian|\n",
      "|10009|    japanese|    japanese|    japanese|\n",
      "|10025|      korean|     chinese|        thai|\n",
      "|10029|    japanese|     chinese|        thai|\n",
      "|10032|     mexican|     mexican|     mexican|\n",
      "|10033|     british| southern_us| southern_us|\n",
      "|10034|     chinese|     chinese|     chinese|\n",
      "|10035|     italian|     italian|     italian|\n",
      "|10037|      korean|  vietnamese|     chinese|\n",
      "|10038|      french|     italian|     italian|\n",
      "| 1004|  vietnamese|     british| southern_us|\n",
      "|10040|     italian|     italian|     italian|\n",
      "|10042| southern_us| southern_us| southern_us|\n",
      "|10043| southern_us| southern_us| southern_us|\n",
      "|10047|      french|      indian|     chinese|\n",
      "|10049|        thai|        thai|     chinese|\n",
      "|10055|     chinese|     chinese|     chinese|\n",
      "| 1006|     chinese|     chinese|     chinese|\n",
      "|10062| southern_us| southern_us|     italian|\n",
      "|10065|     mexican|     mexican|     mexican|\n",
      "| 1007|       greek|     italian|     italian|\n",
      "|10078|      french|      french|     italian|\n",
      "|10082|     italian|     italian|     italian|\n",
      "|10085|      french|     italian|     italian|\n",
      "|10086|     italian|     italian|     italian|\n",
      "|10087|  vietnamese|     spanish| southern_us|\n",
      "|10090| southern_us|    moroccan| southern_us|\n",
      "|10091|     mexican|     italian|     italian|\n",
      "| 1010| southern_us|     italian|cajun_creole|\n",
      "|10108|      indian|      indian|      indian|\n",
      "|10114| southern_us| southern_us| southern_us|\n",
      "|10120|      indian|      indian|      indian|\n",
      "|10122|      french|      french| southern_us|\n",
      "|10133|     italian|     italian|     italian|\n",
      "|10139|     italian|     italian| southern_us|\n",
      "|10140|      french| southern_us| southern_us|\n",
      "|10152|     mexican|     mexican|     mexican|\n",
      "|10154|   brazilian| southern_us| southern_us|\n",
      "|10164|      indian|     chinese|      indian|\n",
      "|10165|     mexican|   brazilian| southern_us|\n",
      "|10168|     italian| southern_us| southern_us|\n",
      "|10175|     mexican|     mexican|     mexican|\n",
      "|10182|     italian|      french|     italian|\n",
      "|10188|      indian|    japanese|     chinese|\n",
      "|10194|      french|     italian|     italian|\n",
      "|10203|      french|     italian|      french|\n",
      "|10204|     italian|     italian|     italian|\n",
      "|10215|        thai|     chinese|     chinese|\n",
      "|10217|     mexican|     mexican|     mexican|\n",
      "|10219|       greek|       greek|     italian|\n",
      "|10220|     italian|     italian|     italian|\n",
      "|10223|     italian|     italian|     italian|\n",
      "|10225|     mexican|     spanish|     italian|\n",
      "|10226|     chinese|     chinese|     chinese|\n",
      "|10227|      indian|      indian|      indian|\n",
      "|10228|    moroccan|      indian|      indian|\n",
      "|10239|     mexican|     mexican|     mexican|\n",
      "|10241|       irish|      french|     italian|\n",
      "|10242|     mexican|     mexican|     mexican|\n",
      "|10247|     mexican|     mexican|     mexican|\n",
      "|10256|    japanese|     chinese|    japanese|\n",
      "|10258|      indian|      indian|      indian|\n",
      "|10260| southern_us|      french|     italian|\n",
      "|10264|      french|      french|     italian|\n",
      "|10265|    moroccan|     italian|     italian|\n",
      "|10275| southern_us| southern_us| southern_us|\n",
      "| 1028| southern_us| southern_us|     mexican|\n",
      "|10285|     mexican|     mexican|     mexican|\n",
      "|10293|     chinese|     chinese|     chinese|\n",
      "|10298|      indian|      indian|      indian|\n",
      "|10303| southern_us|     mexican|     mexican|\n",
      "|10305|     mexican|     mexican|     mexican|\n",
      "| 1031|      indian|      indian|      indian|\n",
      "|10310|     italian|     italian|     italian|\n",
      "|10312|     italian|     italian|     italian|\n",
      "|10323|     italian|     italian|     italian|\n",
      "|10324|     mexican|     mexican|     mexican|\n",
      "|10327|      french|      french|      french|\n",
      "| 1033|    moroccan|    moroccan|      indian|\n",
      "|10335|     mexican|     mexican|     mexican|\n",
      "|10337|       greek|     italian|     italian|\n",
      "|10341|    moroccan|    moroccan|    moroccan|\n",
      "|10344| southern_us|     chinese|     chinese|\n",
      "|10347|      french|       greek| southern_us|\n",
      "|10350|     italian|     italian|     italian|\n",
      "|10351|     italian|     italian|     italian|\n",
      "| 1036|     chinese|     chinese|     chinese|\n",
      "|10364|     british|     italian|     italian|\n",
      "|10365|     chinese|     chinese|    filipino|\n",
      "|10371|     mexican|     mexican|     mexican|\n",
      "|10372|cajun_creole| southern_us| southern_us|\n",
      "|10377|     chinese|     chinese|     chinese|\n",
      "|10385|        thai|        thai|        thai|\n",
      "|10391|     italian|     italian|     italian|\n",
      "|10394|      indian|      indian|      indian|\n",
      "|10402|     chinese|     chinese|     chinese|\n",
      "|10409|cajun_creole|cajun_creole|cajun_creole|\n",
      "+-----+------------+------------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data1.select(\"id\", \"cuisine\", \"y_hat\", \"y_hatRFC\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set de Pruebas Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read parquet... Failed!!!\n",
      "Reading from JSON... Done\n"
     ]
    }
   ],
   "source": [
    "test = None\n",
    "try:\n",
    "    print(\"Trying to read parquet...\", end=\"\")\n",
    "    test = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/test.parquet\")\n",
    "    print(\" OK!\")\n",
    "except:\n",
    "    print(\" Failed!!!\\nReading from JSON...\", end=\"\")\n",
    "    schema_ingredientes_test = schema=StructType().\\\n",
    "        add(\"id\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "        add(\"ingredients\", data_type=ArrayType(StringType()), nullable=True, metadata=None)\n",
    "    test = spark.read.json(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/test.json\", \n",
    "                            schema=schema_ingredientes_test,\n",
    "                            allowUnquotedFieldNames=True,\n",
    "                            multiLine=True)\n",
    "    test = test \\\n",
    "        .withColumn(\"ingreds\", \n",
    "                    col(\"ingredients\").cast(StringType())) \\\n",
    "        .withColumn(\"ingredientes\",\n",
    "                   regexp_replace(col(\"ingreds\"), pattern=\"[\\[\\]]\", replacement=\"\"))\\\n",
    "        .select(\"id\", col(\"ingredientes\").alias(\"ingredients\"))\n",
    "    test.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/test.parquet\", mode=\"overwrite\")\n",
    "    test = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/test.parquet\")\n",
    "    print(\" Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de adecuación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|            features|\n",
      "+-----+--------------------+\n",
      "|18009|(262144,[45688,10...|\n",
      "|28583|(262144,[46588,52...|\n",
      "|41580|(262144,[80021,99...|\n",
      "|29752|(262144,[1176,263...|\n",
      "|35687|(262144,[30649,39...|\n",
      "|38527|(262144,[53031,63...|\n",
      "|19666|(262144,[174095,2...|\n",
      "|41217|(262144,[6113,439...|\n",
      "|28753|(262144,[9790,207...|\n",
      "|22659|(262144,[6767,130...|\n",
      "|21749|(262144,[6113,726...|\n",
      "|44967|(262144,[9879,141...|\n",
      "|42969|(262144,[108254,1...|\n",
      "|44883|(262144,[4065,296...|\n",
      "|20827|(262144,[94567,10...|\n",
      "|23196|(262144,[21625,55...|\n",
      "|35387|(262144,[7977,135...|\n",
      "|33780|(262144,[3419,204...|\n",
      "|19001|(262144,[37601,63...|\n",
      "|16526|(262144,[17468,43...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "## IDF!!!\n",
    "test2 = pipe_hashm.transform(model_data.transform(test)).select(\"id\", \"features\").cache()\n",
    "test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test2 = model_data.transform(test).select(\"id\", \"features\").cache()\n",
    "test2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|   id|     cuisine|\n",
      "+-----+------------+\n",
      "|18009|     british|\n",
      "|28583| southern_us|\n",
      "|41580|     italian|\n",
      "|29752|cajun_creole|\n",
      "|35687|     italian|\n",
      "|38527| southern_us|\n",
      "|19666| southern_us|\n",
      "|41217|     chinese|\n",
      "|28753|     mexican|\n",
      "|22659|     russian|\n",
      "|21749|     italian|\n",
      "|44967|       greek|\n",
      "|42969|      indian|\n",
      "|44883|     italian|\n",
      "|20827|     british|\n",
      "|23196|     italian|\n",
      "|35387|     mexican|\n",
      "|33780|     mexican|\n",
      "|19001|     mexican|\n",
      "|16526|      korean|\n",
      "+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 55.2 ms, sys: 17.1 ms, total: 72.3 ms\n",
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_out = model_ml.transform(test2)\n",
    "test_out2 = test_out.select(\"id\", test_out.y_hat.alias(\"cuisine\"))\n",
    "test_out2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_out2\n",
    " .coalesce(1)\n",
    " .write\n",
    " .csv(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/test_submit.csv\", \n",
    "     header=True,\n",
    "     mode=\"overwrite\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b'], {'c': 'c', 'd': 'd'})"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_star(*args, **kwargs):\n",
    "    return ([x for x in args], \n",
    "            kwargs)\n",
    "    \n",
    "test_star('a', 'b', c='c', d='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /Prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin del Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "* [Notas del Curso Métodos Analíticos, Luis Felipe González, ITAM Primavera 2018](https://clever-mestorf-ee3f54.netlify.com)\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/example/model-downloader/ModelDownloaderExample.ipynb>\n",
    "* <https://nlp.johnsnowlabs.com/components.html>\n",
    "* <https://nlp.johnsnowlabs.com/notebooks.html>\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/1.5.0/python/example/vivekn-sentiment/sentiment.ipynb>\n",
    "* [Indix - Lessons from Using Spark to Process Large Amounts of Data – Part I. Retrieved 2018-05-14](https://www.indix.com/blog/engineering/lessons-from-using-spark-to-process-large-amounts-of-data-part-i/)\n",
    "* [Spark NLP - Dependencies](https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp_2.11/1.5.3)\n",
    "* [StackOverflow: Troubleshotting on Spark](https://stackoverflow.com/a/36903019/7323086)\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/issues/106>\n",
    "* <https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
