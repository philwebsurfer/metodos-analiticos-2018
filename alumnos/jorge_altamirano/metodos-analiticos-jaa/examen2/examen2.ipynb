{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen 2\n",
    "\n",
    "_ 175904 - Jorge III Altamirano Astorga_\n",
    "\n",
    "_176576 - Rodrigo Cedeño_\n",
    "\n",
    "_177508 - Uriel Miranda Miñón_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import time, os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.worker.cleanup.appDataTtl\", 24*60*60)\n",
    "conf.set(\"spark.worker.cleanup.enabled\", True)\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.driver.cores\", 5)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.cores\", 5)\n",
    "conf.set(\"spark.jars\", \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.3.jar,\" +\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/config-1.3.0.jar,\" + #needed nlp\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar,\" + #needed by aws\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/commons-cli-1.2.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar\") #needed by aws\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "### get they creds to login to AWS :-)\n",
    "HOME = os.environ[\"HOME\"]\n",
    "aws_id, aws_key = (None, None)\n",
    "with open(HOME+\"/.aws/credentials\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"aws_access_key_id\" in line:\n",
    "            aws_id = re.sub(\"^.*aws_access_key_id\\s*=\\s*\", \"\", line)\n",
    "        elif \"aws_secret_access_key\" in line:\n",
    "            aws_key = re.sub(\"^.*aws_secret_access_key\\s*=\\s*\", \"\", line)\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_id)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_key)\n",
    "aws_id, aws_key = (None, None)\n",
    "conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.3\")\n",
    "sc = SparkContext(master = \"spark://jupyter.corp.penoles.mx:7077\", \n",
    "                  sparkHome=\"/usr/local/spark/\",\n",
    "                  appName=\"examen-ma-2\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Show:\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[romaine lettuce,...|\n",
      "|25693|southern_us|[plain flour, gro...|\n",
      "|20130|   filipino|[eggs, pepper, sa...|\n",
      "|22213|     indian|[water, vegetable...|\n",
      "|13162|     indian|[black pepper, sh...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 9.64 ms, sys: 9.4 ms, total: 19 ms\n",
      "Wall time: 9.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "schema_ingredientes = schema=StructType().\\\n",
    "    add(\"id\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"cuisine\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"ingredients\", data_type=ArrayType(StringType()), nullable=True, metadata=None)\n",
    "train = spark.read.json(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train.json\", \n",
    "                        schema=schema_ingredientes,\n",
    "                        allowUnquotedFieldNames=True,\n",
    "                        multiLine=True)\n",
    "print(\"Schema:\")\n",
    "train.printSchema()\n",
    "print(\"Show:\")\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de Registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de la Columna\n",
    "\n",
    "Quitamos los arrays, para operar mejor el machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train\\\n",
    "    .withColumn(\"ingreds\", \n",
    "                col(\"ingredients\").cast(StringType()))\\\n",
    "    .withColumn(\"ingredientes\",\n",
    "               regexp_replace(col(\"ingreds\"), pattern=\"[\\[\\]]\", replacement=\"\"))\\\n",
    "    .select(\"id\", \"cuisine\", col(\"ingredientes\").alias(\"ingredients\"))\n",
    "train2.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|\n",
      "|25693|southern_us|plain flour, grou...|\n",
      "|20130|   filipino|eggs, pepper, sal...|\n",
      "|22213|     indian|water, vegetable ...|\n",
      "|13162|     indian|black pepper, sha...|\n",
      "| 6602|   jamaican|plain flour, suga...|\n",
      "|42779|    spanish|olive oil, salt, ...|\n",
      "| 3735|    italian|sugar, pistachio ...|\n",
      "|16903|    mexican|olive oil, purple...|\n",
      "|12734|    italian|chopped tomatoes,...|\n",
      "| 5875|    italian|pimentos, sweet p...|\n",
      "|45887|    chinese|low sodium soy sa...|\n",
      "| 2698|    italian|Italian parsley l...|\n",
      "|41995|    mexican|ground cinnamon, ...|\n",
      "|31908|    italian|fresh parmesan ch...|\n",
      "|24717|     indian|tumeric, vegetabl...|\n",
      "|34466|    british|greek yogurt, lem...|\n",
      "| 1420|    italian|italian seasoning...|\n",
      "| 2941|       thai|sugar, hot chili,...|\n",
      "| 8152| vietnamese|soy sauce, vegeta...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train2 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\")\n",
    "train2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de librerías Spark NLP \n",
    "\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/issues/106>\n",
    "* <https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup sparknlp source\n",
    "## \n",
    "## https://github.com/JohnSnowLabs/spark-nlp/issues/106\n",
    "## https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error\n",
    "import os, glob, sys\n",
    "sys.path.extend(glob.glob(\"/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.0.jar\"))\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "import os, glob, sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      "\n",
      "showing results...\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id   |cuisine    |ingredients                                                                                                                        |\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10259|greek      |romain@lettuc@,@black@oliv@,@grape@tomato@,@garlic@,@pepper@,@purpl@onion@,@season@,@garbanzo@bean@,@feta@chees@crumbl             |\n",
      "|25693|southern_us|plain@flour@,@ground@pepper@,@salt@,@tomato@,@ground@black@pepper@,@thym@,@egg@,@green@tomato@,@yellow@corn@meal@,@milk@,@veget@oil|\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 63.7 ms, sys: 10.5 ms, total: 74.2 ms\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docAssemblr = DocumentAssembler()\\\n",
    "  .setInputCol(\"ingredients\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizr = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")#    .addInfixPattern(\"(\\p{L}+)(n't\\b)\") \\\n",
    "    \n",
    "normalizr = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setPattern(\"[^A-Za-z,]\")\n",
    "    \n",
    "stemmr = Stemmer() \\\n",
    "  .setInputCols([\"normalized\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "    \n",
    "finishr = Finisher() \\\n",
    "    .setInputCols([\"stems\"]) \\\n",
    "    .setOutputCols([\"ingredients\"]) \\\n",
    "    .setIncludeKeys(False)\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    normalizr,\n",
    "    stemmr,\n",
    "    finishr\n",
    "])\n",
    "\n",
    "train.cache()\n",
    "model = pipeline.fit(train2)\n",
    "train3 = model.transform(train2)\n",
    "train3.printSchema()\n",
    "print(\"showing results...\")\n",
    "train3.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canastas\n",
    "\n",
    "En este primer paso lo que realizamos es volver a pasar los datos a array, para que sean canastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 ms, sys: 17 ms, total: 50.6 ms\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "udf_ingredients = udf(lambda ingredients: \n",
    "                      list(set(ingredients)), \n",
    "                      returnType=ArrayType(StringType()))\n",
    "train4 = train3 \\\n",
    "    .withColumn(\"ingredients\", regexp_replace(\"ingredients\", \"@?,@?\", \",\")) \\\n",
    "    .select(\"id\", \"cuisine\",\n",
    "        split(\"ingredients\", \"\\s*,\\s*\").alias(\"ingredients\")) \\\n",
    "    .cache() \\\n",
    "    .withColumn(\"ingredients\", udf_ingredients(\"ingredients\"))    \n",
    "#.select( \\\n",
    "#        \"id\",\n",
    "#        \"cuisine\",\n",
    "#        regexp_replace(\"ingredients\", \"\\@\", \"@ \").alias(\"ingredients\")\\\n",
    "#    ) \\\n",
    "train4.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/tmp-train4.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[purpl@onion, bla...|\n",
      "|25693|southern_us|[veget@oil, salt,...|\n",
      "|20130|   filipino|[chicken@liver, g...|\n",
      "|22213|     indian|[water, veget@oil...|\n",
      "|13162|     indian|[water, cayenn@pe...|\n",
      "| 6602|   jamaican|[ground@ginger, f...|\n",
      "|42779|    spanish|[flat@leaf@parsle...|\n",
      "| 3735|    italian|[flour, white@alm...|\n",
      "|16903|    mexican|[iceberg@lettuc, ...|\n",
      "|12734|    italian|[flat@leaf@parsle...|\n",
      "| 5875|    italian|[mushroom, canola...|\n",
      "|45887|    chinese|[crush@red@pepper...|\n",
      "| 2698|    italian|[italian@parslei@...|\n",
      "|41995|    mexican|[avocado, crush@r...|\n",
      "|31908|    italian|[allpurpos@flour,...|\n",
      "|24717|     indian|[spinach, sweet@p...|\n",
      "|34466|    british|[confection@sugar...|\n",
      "| 1420|    italian|[italian@season, ...|\n",
      "| 2941|       thai|[asian@fish@sauc,...|\n",
      "| 8152| vietnamese|[veget@oil, chick...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train4 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/tmp-train4.parquet\")\n",
    "train4.printSchema()\n",
    "train4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 ms, sys: 1.62 ms, total: 19.1 ms\n",
      "Wall time: 6.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fp = FPGrowth(minSupport=0.1, minConfidence=0.2, itemsCol=\"ingredients\")\n",
    "fpm = fp.fit(train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|items                |freq |\n",
      "+---------------------+-----+\n",
      "|[salt]               |18048|\n",
      "|[onion]              |7972 |\n",
      "|[oliv@oil]           |7971 |\n",
      "|[water]              |7457 |\n",
      "|[garlic]             |7380 |\n",
      "|[sugar]              |6434 |\n",
      "|[garlic@clove]       |6236 |\n",
      "|[butter]             |4847 |\n",
      "|[ground@black@pepper]|4784 |\n",
      "|[allpurpos@flour]    |4632 |\n",
      "|[pepper]             |4438 |\n",
      "|[onion, salt]        |4392 |\n",
      "|[veget@oil]          |4385 |\n",
      "|[oliv@oil, salt]     |4177 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.freqItemsets.orderBy(col(\"freq\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reglas de Asociación y Predicciones\n",
    "\n",
    "Al parecer no se observan reglas de asociación. Por ende, no hay predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|antecedent|consequent|         confidence|\n",
      "+----------+----------+-------------------+\n",
      "|    [salt]|[oliv@oil]|0.23143838652482268|\n",
      "|    [salt]|   [onion]|0.24335106382978725|\n",
      "|[oliv@oil]|    [salt]| 0.5240245891356166|\n",
      "|   [onion]|    [salt]| 0.5509282488710486|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|id   |cuisine    |ingredients                                                                                                                                                                                                                               |prediction       |\n",
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|10259|greek      |[purpl@onion, black@oliv, season, romain@lettuc, pepper, garlic, feta@chees@crumbl, grape@tomato, garbanzo@bean]                                                                                                                          |[]               |\n",
      "|25693|southern_us|[veget@oil, salt, green@tomato, milk, tomato, thym, egg, yellow@corn@meal, ground@pepper, ground@black@pepper, plain@flour]                                                                                                               |[oliv@oil, onion]|\n",
      "|20130|filipino   |[chicken@liver, grill@chicken@breast, mayonais, salt, yellow@onion, cook@oil, pepper, egg, garlic@powder, soi@sauc, green@chili, butter]                                                                                                  |[oliv@oil, onion]|\n",
      "|22213|indian     |[water, veget@oil, salt, wheat]                                                                                                                                                                                                           |[oliv@oil, onion]|\n",
      "|13162|indian     |[water, cayenn@pepper, doubl@cream, black@pepper, boneless@chicken@skinless@thigh, lemon@juic, natur@yogurt, ground@cumin, chili@powder, garlic@past, milk, salt, bai@leaf, oil, passata, cornflour, shallot, butter, onion, garam@masala]|[oliv@oil]       |\n",
      "|6602 |jamaican   |[ground@ginger, fresh@ginger@root, sugar, salt, milk, egg, vanilla@extract, powder@sugar, plain@flour, butter, bake@powder, ground@cinnamon]                                                                                              |[oliv@oil, onion]|\n",
      "|42779|spanish    |[flat@leaf@parslei, skirt@steak, medium@shrimp, oliv@oil, white@vinegar, salt, bai@leaf, pepper, garlic, chop@cilantro, jalapeno@chili, sea@salt, chorizo@sausag]                                                                         |[onion]          |\n",
      "|3735 |italian    |[flour, white@almond@bark, almond@extract, pistachio@nut, sugar, oliv@oil, egg, vanilla@extract, dri@cranberri, bake@powder]                                                                                                              |[salt]           |\n",
      "|16903|mexican    |[iceberg@lettuc, cheddar@chees, purpl@onion, oliv@oil, lime, salt, fresh@pineappl, corn@tortilla, jalapeno@chili, poblano@pepper, ground@black@pepper, pork, chop@cilantro@fresh]                                                         |[onion]          |\n",
      "|12734|italian    |[flat@leaf@parslei, kosher@salt, fresh@basil, extravirgin@oliv@oil, garlic, chop@tomato]                                                                                                                                                  |[]               |\n",
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(train4).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos con ingredientes arbitrarios\n",
    "* **Salt**, Eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "| ingredients|       prediction|\n",
      "+------------+-----------------+\n",
      "|[salt, eggs]|[oliv@oil, onion]|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"salt\", \"eggs\"], )], [\"ingredients\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['old@el@paso@mild@red@enchilada@sauc',\n",
       " 'cook@chicken',\n",
       " 'mexican@chees@blend',\n",
       " 'pillsburi@refriger@crescent@dinner@roll',\n",
       " 'red@enchilada@sauc',\n",
       " 'refriger@crescent@roll']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.registerDataFrameAsTable(train3, \"train3\")\n",
    "spark.registerDataFrameAsTable(train4, \"train4\")\n",
    "ingredients_nonunique = spark\\\n",
    "    .sql(\"SELECT ingredients FROM train4 WHERE id = 1667 OR \\\n",
    "         array_contains(ingredients, 'old@ el@ paso@ mild@ red@ enchilada@ sauc@ ')\") \\\n",
    "    .collect()[0].ingredients\n",
    "ingredients_nonunique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lettuce, Tomato, **Olive@oil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------+\n",
      "|ingredients                |prediction|\n",
      "+---------------------------+----------+\n",
      "|[lettuce, tomato, oliv@oil]|[salt]    |\n",
      "+---------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"lettuce\", \"tomato\", \"oliv@oil\"], )], [\"ingredients\"])).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.59 ms, sys: 4.83 ms, total: 10.4 ms\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6681"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train4.select(explode(\"ingredients\")).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Títulos de Wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/sh\r\n",
      "#english español Deutsch portugués francais tagalog italiano vietnamese\r\n",
      "source ~/.bash_profile\r\n",
      "for lang in en es de pt fr lt it vi; \r\n",
      "do\r\n",
      "\techo \"Downloading $lang\"\r\n",
      "\twget -c \"https://dumps.wikimedia.org/${lang}wiki/latest/${lang}wiki-latest-all-titles-in-ns0.gz\"\r\n",
      "\techo -n Transforming $lang...\r\n",
      "\tzcat ${lang}wiki-latest-all-titles-in-ns0.gz | \\\r\n",
      "\t\t sed 's!_\\+! !g;s![^a-z ]!!ig;s!^\\s\\+!!;s!\\s\\+$!!;/^\\s*$/d' | \\\r\n",
      "\t\t tr '[:upper:]' '[:lower:]' | tr ' ' '\\n' | sort -u > \"${lang}wiki-latest-all-titles-in-ns0-transform\"\r\n",
      "\techo \" done!\"\r\n",
      "done\r\n",
      "cat *-transform > wiki-titles.txt\r\n",
      "hdfs dfs -copyFromLocal wiki-titles.txt hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/\r\n"
     ]
    }
   ],
   "source": [
    "!cat wikipedia-title-corpus-download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+--------------------+\n",
      "|   id|    cuisine|         ingredients|        ingredients3|\n",
      "+-----+-----------+--------------------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|romain lettuc , b...|\n",
      "|25693|southern_us|plain flour, grou...|plain flour , gro...|\n",
      "+-----+-----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 102 ms, sys: 26.7 ms, total: 128 ms\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stemmr2 = Stemmer() \\\n",
    "  .setInputCols([\"tokens\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/wiki-titles.txt\"\n",
    "norvig = NorvigSweetingApproach()\n",
    "norvig.setInputCols([\"stems\"])\n",
    "norvig.setOutputCol(\"ingredients2\")\n",
    "norvig.setDictionary(path_dict)\n",
    "finishr2 = Finisher() \\\n",
    "    .setInputCols([\"ingredients2\"]) \\\n",
    "    .setOutputCols([\"ingredients3\"]) \\\n",
    "    .setIncludeKeys(False) \\\n",
    "    .setAnnotationSplitSymbol(\" \")\n",
    "\n",
    "\n",
    "pipeline1 = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    stemmr2,\n",
    "    norvig,\n",
    "    finishr2\n",
    "])\n",
    "model1 = pipeline1.fit(train2)\n",
    "train4 = model1.transform(train2)\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "import pyspark.sql.functions as sparkFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.9 ms, sys: 9.29 ms, total: 72.2 ms\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train5 = train4.withColumn(\"ingredients2\", sparkFunctions.split(\"ingredients3\", \",\")) \\\n",
    "         .cache()\n",
    "word2v = Word2Vec(vectorSize=100, seed=175904, inputCol=\"ingredients2\", outputCol=\"word2vec\")\n",
    "word2vm = word2v.fit(train5)\n",
    "train_w2v = word2vm.transform(train5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|    cuisine|         ingredients|        ingredients3|        ingredients2|            word2vec|\n",
      "+-----+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|romain lettuc , b...|[romain lettuc , ...|[-0.0034980849983...|\n",
      "|25693|southern_us|plain flour, grou...|plain flour , gro...|[plain flour ,  g...|[0.07107551432934...|\n",
      "|20130|   filipino|eggs, pepper, sal...|egg , pepper , sa...|[egg ,  pepper , ...|[0.03438723728080...|\n",
      "|22213|     indian|water, vegetable ...|water , veget oil...|[water ,  veget o...|[0.07553127699065...|\n",
      "|13162|     indian|black pepper, sha...|black pepper , sh...|[black pepper ,  ...|[0.07961993085300...|\n",
      "| 6602|   jamaican|plain flour, suga...|plain flour , sug...|[plain flour ,  s...|[0.12873898710434...|\n",
      "|42779|    spanish|olive oil, salt, ...|oliv oil , salt ,...|[oliv oil ,  salt...|[-0.0062195731756...|\n",
      "| 3735|    italian|sugar, pistachio ...|sugar , pistachio...|[sugar ,  pistach...|[0.03430244047194...|\n",
      "|16903|    mexican|olive oil, purple...|oliv oil , purpl ...|[oliv oil ,  purp...|[-0.0089989627818...|\n",
      "|12734|    italian|chopped tomatoes,...|chop tomato , fre...|[chop tomato ,  f...|[0.02471002637563...|\n",
      "| 5875|    italian|pimentos, sweet p...|pimento , sweet p...|[pimento ,  sweet...|[-0.0240682438911...|\n",
      "|45887|    chinese|low sodium soy sa...|low sodium soi sa...|[low sodium soi s...|[-0.0647938878585...|\n",
      "| 2698|    italian|Italian parsley l...|italian parslei l...|[italian parslei ...|[0.03446181102190...|\n",
      "|41995|    mexican|ground cinnamon, ...|ground cinnamon ,...|[ground cinnamon ...|[0.01670660887954...|\n",
      "|31908|    italian|fresh parmesan ch...|fresh parmesan ch...|[fresh parmesan c...|[0.05624215947076...|\n",
      "|24717|     indian|tumeric, vegetabl...|tumer , veget sto...|[tumer ,  veget s...|[0.07939426787197...|\n",
      "|34466|    british|greek yogurt, lem...|greek yogurt , le...|[greek yogurt ,  ...|[0.03638339189637...|\n",
      "| 1420|    italian|italian seasoning...|italian season , ...|[italian season ,...|[-0.0199193670996...|\n",
      "| 2941|       thai|sugar, hot chili,...|sugar , hot chili...|[sugar ,  hot chi...|[-0.0345859354129...|\n",
      "| 8152| vietnamese|soy sauce, vegeta...|soi sauc , veget ...|[soi sauc ,  vege...|[-0.0360192402552...|\n",
      "+-----+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_w2v.show()\n",
    "train_w2v.coalesce(1).write.json(\"s3a://jorge-altamirano/ma2018-examen2/word2vec-small.json\", \n",
    "                                 compression=\"gzip\",\n",
    "                                mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "* [Notas del Curso Métodos Analíticos, Luis Felipe González, ITAM Primavera 2018](https://clever-mestorf-ee3f54.netlify.com)\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/example/model-downloader/ModelDownloaderExample.ipynb>\n",
    "* <https://nlp.johnsnowlabs.com/components.html>\n",
    "* <https://nlp.johnsnowlabs.com/notebooks.html>\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/1.5.0/python/example/vivekn-sentiment/sentiment.ipynb>\n",
    "* [Indix - Lessons from Using Spark to Process Large Amounts of Data – Part I. Retrieved 2018-05-14](https://www.indix.com/blog/engineering/lessons-from-using-spark-to-process-large-amounts-of-data-part-i/)\n",
    "* [Spark NLP - Dependencies](https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp_2.11/1.5.3)\n",
    "* [StackOverflow: Troubleshotting on Spark](https://stackoverflow.com/a/36903019/7323086)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
