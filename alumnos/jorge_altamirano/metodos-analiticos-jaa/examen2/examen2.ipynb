{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen 2\n",
    "\n",
    "_ 175904 - Jorge III Altamirano Astorga_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.worker.cleanup.appDataTtl\", 24*60*60)\n",
    "conf.set(\"spark.worker.cleanup.enabled\", True)\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.driver.cores\", 14)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.cores\", 14)\n",
    "conf.set(\"spark.jars\", \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.0.jar\")\n",
    "conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.3\")\n",
    "sc = SparkContext(master = \"spark://jupyter.corp.penoles.mx:7077\", \n",
    "                  sparkHome=\"/usr/local/spark/\",\n",
    "                  appName=\"examen-ma-2\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Show:\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[romaine lettuce,...|\n",
      "|25693|southern_us|[plain flour, gro...|\n",
      "|20130|   filipino|[eggs, pepper, sa...|\n",
      "|22213|     indian|[water, vegetable...|\n",
      "|13162|     indian|[black pepper, sh...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 8.9 ms, sys: 2.46 ms, total: 11.4 ms\n",
      "Wall time: 8.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "schema_ingredientes = schema=StructType().\\\n",
    "    add(\"id\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"cuisine\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"ingredients\", data_type=ArrayType(StringType()), nullable=True, metadata=None)\n",
    "train = spark.read.json(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train.json\", \n",
    "                        schema=schema_ingredientes,\n",
    "                        allowUnquotedFieldNames=True,\n",
    "                        multiLine=True)\n",
    "print(\"Schema:\")\n",
    "train.printSchema()\n",
    "print(\"Show:\")\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de Registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de la Columna\n",
    "\n",
    "Quitamos los arrays, para operar mejor el machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train\\\n",
    "    .withColumn(\"ingreds\", \n",
    "                col(\"ingredients\").cast(StringType()))\\\n",
    "    .withColumn(\"ingredientes\",\n",
    "               regexp_replace(col(\"ingreds\"), pattern=\"[\\[\\]]\", replacement=\"\"))\\\n",
    "    .select(\"id\", \"cuisine\", col(\"ingredientes\").alias(\"ingredients\"))\n",
    "train2.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|\n",
      "|25693|southern_us|plain flour, grou...|\n",
      "|20130|   filipino|eggs, pepper, sal...|\n",
      "|22213|     indian|water, vegetable ...|\n",
      "|13162|     indian|black pepper, sha...|\n",
      "| 6602|   jamaican|plain flour, suga...|\n",
      "|42779|    spanish|olive oil, salt, ...|\n",
      "| 3735|    italian|sugar, pistachio ...|\n",
      "|16903|    mexican|olive oil, purple...|\n",
      "|12734|    italian|chopped tomatoes,...|\n",
      "| 5875|    italian|pimentos, sweet p...|\n",
      "|45887|    chinese|low sodium soy sa...|\n",
      "| 2698|    italian|Italian parsley l...|\n",
      "|41995|    mexican|ground cinnamon, ...|\n",
      "|31908|    italian|fresh parmesan ch...|\n",
      "|24717|     indian|tumeric, vegetabl...|\n",
      "|34466|    british|greek yogurt, lem...|\n",
      "| 1420|    italian|italian seasoning...|\n",
      "| 2941|       thai|sugar, hot chili,...|\n",
      "| 8152| vietnamese|soy sauce, vegeta...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train2 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\")\n",
    "train2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de librerías Spark NLP \n",
    "\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/issues/106>\n",
    "* <https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup sparknlp source\n",
    "## \n",
    "## https://github.com/JohnSnowLabs/spark-nlp/issues/106\n",
    "## https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error\n",
    "import os, glob, sys\n",
    "sys.path.extend(glob.glob(\"/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.0.jar\"))\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "import os, glob, sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      "\n",
      "showing results...\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id   |cuisine    |ingredients                                                                                                                        |\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10259|greek      |romain@lettuc@,@black@oliv@,@grape@tomato@,@garlic@,@pepper@,@purpl@onion@,@season@,@garbanzo@bean@,@feta@chees@crumbl             |\n",
      "|25693|southern_us|plain@flour@,@ground@pepper@,@salt@,@tomato@,@ground@black@pepper@,@thym@,@egg@,@green@tomato@,@yellow@corn@meal@,@milk@,@veget@oil|\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docAssemblr = DocumentAssembler()\\\n",
    "  .setInputCol(\"ingredients\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizr = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")#    .addInfixPattern(\"(\\p{L}+)(n't\\b)\") \\\n",
    "    \n",
    "normalizr = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setPattern(\"[^A-Za-z,]\")\n",
    "    \n",
    "stemmr = Stemmer() \\\n",
    "  .setInputCols([\"normalized\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "    \n",
    "finishr = Finisher() \\\n",
    "    .setInputCols([\"stems\"]) \\\n",
    "    .setOutputCols([\"ingredients\"]) \\\n",
    "    .setIncludeKeys(False)\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    normalizr,\n",
    "    stemmr,\n",
    "    finishr\n",
    "])\n",
    "\n",
    "train.cache()\n",
    "model = pipeline.fit(train2)\n",
    "train3 = model.transform(train2)\n",
    "train3.printSchema()\n",
    "print(\"showing results...\")\n",
    "train3.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canastas\n",
    "\n",
    "En este primer paso lo que realizamos es volver a pasar los datos a array, para que sean canastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[purpl@onion, bla...|\n",
      "|25693|southern_us|[veget@oil, salt,...|\n",
      "|20130|   filipino|[chicken@liver, g...|\n",
      "|22213|     indian|[water, veget@oil...|\n",
      "|13162|     indian|[water, cayenn@pe...|\n",
      "| 6602|   jamaican|[ground@ginger, f...|\n",
      "|42779|    spanish|[flat@leaf@parsle...|\n",
      "| 3735|    italian|[flour, white@alm...|\n",
      "|16903|    mexican|[iceberg@lettuc, ...|\n",
      "|12734|    italian|[flat@leaf@parsle...|\n",
      "| 5875|    italian|[mushroom, canola...|\n",
      "|45887|    chinese|[crush@red@pepper...|\n",
      "| 2698|    italian|[italian@parslei@...|\n",
      "|41995|    mexican|[avocado, crush@r...|\n",
      "|31908|    italian|[allpurpos@flour,...|\n",
      "|24717|     indian|[spinach, sweet@p...|\n",
      "|34466|    british|[confection@sugar...|\n",
      "| 1420|    italian|[italian@season, ...|\n",
      "| 2941|       thai|[asian@fish@sauc,...|\n",
      "| 8152| vietnamese|[veget@oil, chick...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 42.6 ms, sys: 10.6 ms, total: 53.1 ms\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "udf_ingredients = udf(lambda ingredients: \n",
    "                      list(set(ingredients)), \n",
    "                      returnType=ArrayType(StringType()))\n",
    "train4 = train3 \\\n",
    "    .withColumn(\"ingredients\", regexp_replace(\"ingredients\", \"@?,@?\", \",\")) \\\n",
    "    .select(\"id\", \"cuisine\",\n",
    "        split(\"ingredients\", \"\\s*,\\s*\").alias(\"ingredients\")) \\\n",
    "    .cache() \\\n",
    "    .withColumn(\"ingredients\", udf_ingredients(\"ingredients\"))    \n",
    "#.select( \\\n",
    "#        \"id\",\n",
    "#        \"cuisine\",\n",
    "#        regexp_replace(\"ingredients\", \"\\@\", \"@ \").alias(\"ingredients\")\\\n",
    "#    ) \\\n",
    "train4.write.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/tmp-train4.parquet\", mode=\"overwrite\")\n",
    "train4 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/tmp-train4.parquet\")\n",
    "train4.printSchema()\n",
    "train4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 ms, sys: 6.84 ms, total: 22 ms\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fp = FPGrowth(minSupport=0.1, minConfidence=0.2, itemsCol=\"ingredients\")\n",
    "fpm = fp.fit(train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|items                |freq |\n",
      "+---------------------+-----+\n",
      "|[salt]               |18048|\n",
      "|[onion]              |7972 |\n",
      "|[oliv@oil]           |7971 |\n",
      "|[water]              |7457 |\n",
      "|[garlic]             |7380 |\n",
      "|[sugar]              |6434 |\n",
      "|[garlic@clove]       |6236 |\n",
      "|[butter]             |4847 |\n",
      "|[ground@black@pepper]|4784 |\n",
      "|[allpurpos@flour]    |4632 |\n",
      "|[pepper]             |4438 |\n",
      "|[onion, salt]        |4392 |\n",
      "|[veget@oil]          |4385 |\n",
      "|[oliv@oil, salt]     |4177 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.freqItemsets.orderBy(col(\"freq\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reglas de Asociación y Predicciones\n",
    "\n",
    "Al parecer no se observan reglas de asociación. Por ende, no hay predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|antecedent|consequent|         confidence|\n",
      "+----------+----------+-------------------+\n",
      "|    [salt]|[oliv@oil]|0.23143838652482268|\n",
      "|    [salt]|   [onion]|0.24335106382978725|\n",
      "|[oliv@oil]|    [salt]| 0.5240245891356166|\n",
      "|   [onion]|    [salt]| 0.5509282488710486|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|id   |cuisine    |ingredients                                                                                                                                                                                                                               |prediction       |\n",
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "|10259|greek      |[purpl@onion, black@oliv, season, romain@lettuc, pepper, garlic, feta@chees@crumbl, grape@tomato, garbanzo@bean]                                                                                                                          |[]               |\n",
      "|25693|southern_us|[veget@oil, salt, green@tomato, milk, tomato, thym, egg, yellow@corn@meal, ground@pepper, ground@black@pepper, plain@flour]                                                                                                               |[oliv@oil, onion]|\n",
      "|20130|filipino   |[chicken@liver, grill@chicken@breast, mayonais, salt, yellow@onion, cook@oil, pepper, egg, garlic@powder, soi@sauc, green@chili, butter]                                                                                                  |[oliv@oil, onion]|\n",
      "|22213|indian     |[water, veget@oil, salt, wheat]                                                                                                                                                                                                           |[oliv@oil, onion]|\n",
      "|13162|indian     |[water, cayenn@pepper, doubl@cream, black@pepper, boneless@chicken@skinless@thigh, lemon@juic, natur@yogurt, ground@cumin, chili@powder, garlic@past, milk, salt, bai@leaf, oil, passata, cornflour, shallot, butter, onion, garam@masala]|[oliv@oil]       |\n",
      "|6602 |jamaican   |[ground@ginger, fresh@ginger@root, sugar, salt, milk, egg, vanilla@extract, powder@sugar, plain@flour, butter, bake@powder, ground@cinnamon]                                                                                              |[oliv@oil, onion]|\n",
      "|42779|spanish    |[flat@leaf@parslei, skirt@steak, medium@shrimp, oliv@oil, white@vinegar, salt, bai@leaf, pepper, garlic, chop@cilantro, jalapeno@chili, sea@salt, chorizo@sausag]                                                                         |[onion]          |\n",
      "|3735 |italian    |[flour, white@almond@bark, almond@extract, pistachio@nut, sugar, oliv@oil, egg, vanilla@extract, dri@cranberri, bake@powder]                                                                                                              |[salt]           |\n",
      "|16903|mexican    |[iceberg@lettuc, cheddar@chees, purpl@onion, oliv@oil, lime, salt, fresh@pineappl, corn@tortilla, jalapeno@chili, poblano@pepper, ground@black@pepper, pork, chop@cilantro@fresh]                                                         |[onion]          |\n",
      "|12734|italian    |[flat@leaf@parslei, kosher@salt, fresh@basil, extravirgin@oliv@oil, garlic, chop@tomato]                                                                                                                                                  |[]               |\n",
      "+-----+-----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(train4).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos con ingredientes arbitrarios\n",
    "* **Salt**, Eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "| ingredients|       prediction|\n",
      "+------------+-----------------+\n",
      "|[salt, eggs]|[oliv@oil, onion]|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"salt\", \"eggs\"], )], [\"ingredients\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['old@el@paso@mild@red@enchilada@sauc',\n",
       " 'cook@chicken',\n",
       " 'mexican@chees@blend',\n",
       " 'pillsburi@refriger@crescent@dinner@roll',\n",
       " 'red@enchilada@sauc',\n",
       " 'refriger@crescent@roll']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.registerDataFrameAsTable(train3, \"train3\")\n",
    "spark.registerDataFrameAsTable(train4, \"train4\")\n",
    "ingredients_nonunique = spark\\\n",
    "    .sql(\"SELECT ingredients FROM train4 WHERE id = 1667 OR \\\n",
    "         array_contains(ingredients, 'old@ el@ paso@ mild@ red@ enchilada@ sauc@ ')\") \\\n",
    "    .collect()[0].ingredients\n",
    "ingredients_nonunique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lettuce, Tomato, **Olive@oil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------+\n",
      "|ingredients                |prediction|\n",
      "+---------------------------+----------+\n",
      "|[lettuce, tomato, oliv@oil]|[salt]    |\n",
      "+---------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"lettuce\", \"tomato\", \"oliv@oil\"], )], [\"ingredients\"])).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 ms, sys: 1.03 ms, total: 9.22 ms\n",
      "Wall time: 2.62 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6681"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train4.select(explode(\"ingredients\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 9)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/intel/intelpython3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-3-2dc1c90849af>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '# norvig = NorvigSweetingModel().pretrained(name=\"spell_fast\", language=\"en\")\\npath_dict = \"file:/home/jaa6766/enwiki-latest-all-titles-in-ns0-transform\"\\n# path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\"\\nnorvig = NorvigSweetingApproach()\\nnorvig.setInputCols([\"tokens\"])\\nnorvig.setOutputCol(\"ingredients2\")\\n# norvig.setCorpus(\"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\")\\nnorvig.setDictionary(path_dict)\\n%%time\\n# norvig = NorvigSweetingModel().pretrained(name=\"spell_fast\", language=\"en\")\\npath_dict = \"file:/home/jaa6766/enwiki-latest-all-titles-in-ns0-transform\"\\n# path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\"\\nnorvig = NorvigSweetingApproach()\\nnorvig.setInputCols([\"tokens\"])\\nnorvig.setOutputCol(\"ingredients2\")\\n# norvig.setCorpus(\"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\")\\nnorvig.setDictionary(path_dict)\\npipeline1 = Pipeline(stages = [\\n    docAssemblr,\\n    tokenizr, \\n    norvig\\n#     normalizr,\\n#     stemmr,\\n#     finishr\\n])\\nmodel1 = pipeline1.fit(train2)\\ntrain4 = model1.transform(train2)\\ntrain4.show(2)\\npipeline1 = Pipeline(stages = [\\n    docAssemblr,\\n    tokenizr, \\n    norvig\\n#     normalizr,\\n#     stemmr,\\n#     finishr\\n])\\nmodel1 = pipeline1.fit(train2)\\ntrain4 = model1.transform(train2)\\ntrain4.show(2)')\n",
      "  File \u001b[1;32m\"/opt/intel/intelpython3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2103\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[1;32m\"<decorator-gen-62>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/opt/intel/intelpython3/lib/python3.6/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/opt/intel/intelpython3/lib/python3.6/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1179\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/intel/intelpython3/lib/python3.6/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m99\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    %%time\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# norvig = NorvigSweetingModel().pretrained(name=\"spell_fast\", language=\"en\")\n",
    "path_dict = \"file:/home/jaa6766/enwiki-latest-all-titles-in-ns0-transform\"\n",
    "# path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\"\n",
    "norvig = NorvigSweetingApproach()\n",
    "norvig.setInputCols([\"tokens\"])\n",
    "norvig.setOutputCol(\"ingredients2\")\n",
    "# norvig.setCorpus(\"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\")\n",
    "norvig.setDictionary(path_dict)\n",
    "pipeline1 = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    norvig\n",
    "#     normalizr,\n",
    "#     stemmr,\n",
    "#     finishr\n",
    "])\n",
    "model1 = pipeline1.fit(train2)\n",
    "train4 = model1.transform(train2)\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Borrar\n",
    "\n",
    "Pruebas norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|      _c0|                 _c1|\n",
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "|       UA|United Air Lines ...|\n",
      "|       AA|American Airlines...|\n",
      "|       US|     US Airways Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "|       B6|     JetBlue Airways|\n",
      "|       OO|Skywest Airlines ...|\n",
      "|       AS|Alaska Airlines Inc.|\n",
      "|       NK|    Spirit Air Lines|\n",
      "|       WN|Southwest Airline...|\n",
      "|       DL|Delta Air Lines Inc.|\n",
      "|       EV|Atlantic Southeas...|\n",
      "|       HA|Hawaiian Airlines...|\n",
      "|       MQ|American Eagle Ai...|\n",
      "|       VX|      Virgin America|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlines = spark.read.csv(\"s3a://jorge-altamirano/flights/airlines.csv\")\n",
    "airlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+--------------------+\n",
      "|   id|cuisine|         ingredients|\n",
      "+-----+-------+--------------------+\n",
      "|10259|  greek|romaine lettuce, ...|\n",
      "+-----+-------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "import time, os, re, glob, sys\n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.worker.cleanup.appDataTtl\", 24*60*60)\n",
    "conf.set(\"spark.worker.cleanup.enabled\", True)\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.driver.cores\", 5)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.cores\", 5)\n",
    "conf.set(\"spark.jars\", \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.3.jar,\" +\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/config-1.3.0.jar,\" + #needed nlp\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar,\" + #needed by aws\n",
    "         \"local:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/commons-cli-1.2.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/hadoop-aws-2.7.3.jar,\" + #needed by aws\n",
    "         \"file:/usr/local/spark-2.3.0-bin-hadoop2.7/jars/aws-java-sdk-1.7.4.jar\") #needed by aws\n",
    "conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.3\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "### get they creds to login to AWS :-)\n",
    "HOME = os.environ[\"HOME\"]\n",
    "aws_id, aws_key = (None, None)\n",
    "with open(HOME+\"/.aws/credentials\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \"aws_access_key_id\" in line:\n",
    "            aws_id = re.sub(\"^.*aws_access_key_id\\s*=\\s*\", \"\", line)\n",
    "        elif \"aws_secret_access_key\" in line:\n",
    "            aws_key = re.sub(\"^.*aws_secret_access_key\\s*=\\s*\", \"\", line)\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", aws_id)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", aws_key)\n",
    "aws_id, aws_key = (None, None)\n",
    "### end getting keys\n",
    "sc = SparkContext(master = \"spark://jupyter.corp.penoles.mx:7077\", \n",
    "                  sparkHome=\"/usr/local/spark/\",\n",
    "                  appName=\"examen-ma-2\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "## setup sparknlp source\n",
    "## \n",
    "## https://github.com/JohnSnowLabs/spark-nlp/issues/106\n",
    "## https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error\n",
    "sys.path.extend(glob.glob(\"/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-nlp_2.11-1.5.3.jar\"))\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "train2 = spark.read.parquet(\"hdfs://jupyter.corp.penoles.mx:9000/ma2018-examen2/train2.parquet\").cache()\n",
    "train2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+--------------------+\n",
      "|   id|    cuisine|         ingredients|        ingredients3|\n",
      "+-----+-----------+--------------------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|romain lettuc , b...|\n",
      "|25693|southern_us|plain flour, grou...|plain flour , gro...|\n",
      "+-----+-----------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "CPU times: user 80.9 ms, sys: 21.7 ms, total: 103 ms\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docAssemblr = DocumentAssembler()\\\n",
    "  .setInputCol(\"ingredients\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizr = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")#    .addInfixPattern(\"(\\p{L}+)(n't\\b)\") \\\n",
    "    \n",
    "normalizr = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setPattern(\"[^A-Za-z,]\")\n",
    "    \n",
    "path_dict = \"file:/home/jaa6766/enwiki-latest-all-titles-in-ns0-transform\"\n",
    "# path_dict = \"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\"\n",
    "norvig = NorvigSweetingApproach() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"ingredients2\") \\\n",
    "    .setDictionary(path_dict)\n",
    "# norvig.setCorpus(\"hdfs://jupyter.corp.penoles.mx:9000/spell-dicts/enwiki-latest-all-titles-in-ns0-transform\")\n",
    "    \n",
    "stemmr2 = Stemmer() \\\n",
    "  .setInputCols([\"ingredients2\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "    \n",
    "finishr2 = Finisher() \\\n",
    "    .setInputCols([\"stems\"]) \\\n",
    "    .setOutputCols([\"ingredients3\"]) \\\n",
    "    .setIncludeKeys(False) \\\n",
    "    .setAnnotationSplitSymbol(\" \")\n",
    "\n",
    "pipeline1 = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    normalizr,\n",
    "    norvig,\n",
    "    stemmr2,\n",
    "    finishr2\n",
    "])\n",
    "model1 = pipeline1.fit(train2)\n",
    "train4 = model1.transform(train2)\n",
    "train4.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ingredients3 | romain lettuc , black oliv , grape tomato , garlic , pepper , purpl onion , season , garbanzo bean , feta chees crumbl                            \n",
      " cuisine      | greek                                                                                                                                             \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ingredients3 | plain flour , ground pepper , salt , tomato , ground black pepper , thym , egg , green tomato , yellow corn meal , milk , veget oil               \n",
      " cuisine      | southern_us                                                                                                                                       \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ingredients3 | egg , pepper , salt , mayonais , cook oil , green chili , grill chicken breast , garlic powder , yellow onion , soi sauc , butter , chicken liver \n",
      " cuisine      | filipino                                                                                                                                          \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " ingredients3 | water , veget oil , wheat , salt                                                                                                                  \n",
      " cuisine      | indian                                                                                                                                            \n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train4.select(\"ingredients3\", \"cuisine\").show(4, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 ms, sys: 2.25 ms, total: 8.82 ms\n",
      "Wall time: 57.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train4 = train4 \\\n",
    "    .select(\"ingredients3\", \"cuisine\") \\\n",
    "    .coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4.write.csv(\"s3a://jorge-altamirano/ma2018-examen2/clean-ingredients.csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## /fin Borrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     cuisine|\n",
      "+------------+\n",
      "|   brazilian|\n",
      "|     british|\n",
      "|cajun_creole|\n",
      "|     chinese|\n",
      "|    filipino|\n",
      "|      french|\n",
      "|       greek|\n",
      "|      indian|\n",
      "|       irish|\n",
      "|     italian|\n",
      "|    jamaican|\n",
      "|    japanese|\n",
      "|      korean|\n",
      "|     mexican|\n",
      "|    moroccan|\n",
      "|     russian|\n",
      "| southern_us|\n",
      "|     spanish|\n",
      "|        thai|\n",
      "|  vietnamese|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o305.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 27 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to /10.10.208.212:42474 \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:519) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:450) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253) \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247) \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830) \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Failed to connect to /10.10.208.212:42474 \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245) \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187) \tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169) \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) \tat java.util.concurrent.FutureTask.run(FutureTask.java:266) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) \t... 1 more Caused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /10.10.208.212:42474 \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) \tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323) \tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \t... 2 more Caused by: java.net.NoRouteToHostException: No route to host \t... 11 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1368)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-88796d7976d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuisine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuisine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuisine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o305.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 27 (count at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to /10.10.208.212:42474 \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:519) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:450) \tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32) \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithoutKey$(Unknown Source) \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source) \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253) \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247) \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830) \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:109) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Failed to connect to /10.10.208.212:42474 \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245) \tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187) \tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:113) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141) \tat org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169) \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) \tat java.util.concurrent.FutureTask.run(FutureTask.java:266) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138) \t... 1 more Caused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: /10.10.208.212:42474 \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) \tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323) \tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) \tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580) \tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497) \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) \t... 2 more Caused by: java.net.NoRouteToHostException: No route to host \t... 11 more \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1368)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1817)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "train2.select(\"cuisine\").distinct().orderBy(\"cuisine\").show()\n",
    "train2.select(\"cuisine\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/sh\r\n",
      "#english español Deutsch portugués francais tagalog italiano vietnamese\r\n",
      "for lang in en es de pt fr lt it vi; \r\n",
      "do\r\n",
      "\techo \"Downloading $lang\"\r\n",
      "\twget -c \"https://dumps.wikimedia.org/${lang}wiki/latest/${lang}wiki-latest-all-titles-in-ns0.gz\"\r\n",
      "\techo -n Transforming $lang...\r\n",
      "\tzcat ${lang}wiki-latest-all-titles-in-ns0.gz | \\\r\n",
      "\t\t sed 's!_\\+! !g;s![^a-z ]!!ig;s!^\\s\\+!!;s!\\s\\+$!!;/^\\s*$/d' | \\\r\n",
      "\t\t tr '[:upper:]' '[:lower:]' | tr ' ' '\\n' | sort -u > \"${lang}wiki-latest-all-titles-in-ns0-transform\"\r\n",
      "\techo \" done!\"\r\n",
      "done\r\n"
     ]
    }
   ],
   "source": [
    "!cat wikipedia-title-corpus-download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin del Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía\n",
    "\n",
    "* [Notas del Curso Métodos Analíticos, Luis Felipe González, ITAM Primavera 2018](https://clever-mestorf-ee3f54.netlify.com)\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/example/model-downloader/ModelDownloaderExample.ipynb>\n",
    "* <https://nlp.johnsnowlabs.com/components.html>\n",
    "* <https://nlp.johnsnowlabs.com/notebooks.html>\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/blob/1.5.0/python/example/vivekn-sentiment/sentiment.ipynb>\n",
    "* [Indix - Lessons from Using Spark to Process Large Amounts of Data – Part I. Retrieved 2018-05-14](https://www.indix.com/blog/engineering/lessons-from-using-spark-to-process-large-amounts-of-data-part-i/)\n",
    "* [Spark NLP - Dependencies](https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp_2.11/1.5.3)\n",
    "* [StackOverflow: Troubleshotting on Spark](https://stackoverflow.com/a/36903019/7323086)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
