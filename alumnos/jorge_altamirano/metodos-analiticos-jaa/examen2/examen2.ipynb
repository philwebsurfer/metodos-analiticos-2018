{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen 2\n",
    "\n",
    "_ 175904 - Jorge III Altamirano Astorga_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrameStatFunctions, DataFrame\n",
    "from pyspark.sql.types import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.driver.cores\", 4)\n",
    "conf.set(\"spark.driver.memoryOverhead\", 0.9)\n",
    "conf.set(\"spark.executor.memory\", \"32g\")\n",
    "conf.set(\"spark.executor.cores\", 12)\n",
    "conf.set(\"spark.jars\", \"/home/jaa6766\")\n",
    "conf.set(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:1.5.0\")\n",
    "sc = SparkContext(master = \"local\", sparkHome=\"/usr/local/spark/\", \n",
    "                  appName=\"examen-ma-2\", conf=conf)\n",
    "spark = SQLContext(sc)\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "Show:\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[romaine lettuce,...|\n",
      "|25693|southern_us|[plain flour, gro...|\n",
      "|20130|   filipino|[eggs, pepper, sa...|\n",
      "|22213|     indian|[water, vegetable...|\n",
      "|13162|     indian|[black pepper, sh...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_ingredientes = schema=StructType().\\\n",
    "    add(\"id\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"cuisine\", data_type=StringType(), nullable=False, metadata=None).\\\n",
    "    add(\"ingredients\", data_type=ArrayType(StringType()), nullable=True, metadata=None)\n",
    "train = spark.read.json(\"hdfs://172.17.0.3:9000/ma2018-examen2/train.json\", \n",
    "                        schema=schema_ingredientes,\n",
    "                        allowUnquotedFieldNames=True,\n",
    "                        multiLine=True)\n",
    "print(\"Schema:\")\n",
    "train.printSchema()\n",
    "print(\"Show:\")\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de Registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39774"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulación de la Columna\n",
    "\n",
    "Quitamos los arrays, para operar mejor el machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|romaine lettuce, ...|\n",
      "|25693|southern_us|plain flour, grou...|\n",
      "|20130|   filipino|eggs, pepper, sal...|\n",
      "|22213|     indian|water, vegetable ...|\n",
      "|13162|     indian|black pepper, sha...|\n",
      "| 6602|   jamaican|plain flour, suga...|\n",
      "|42779|    spanish|olive oil, salt, ...|\n",
      "| 3735|    italian|sugar, pistachio ...|\n",
      "|16903|    mexican|olive oil, purple...|\n",
      "|12734|    italian|chopped tomatoes,...|\n",
      "| 5875|    italian|pimentos, sweet p...|\n",
      "|45887|    chinese|low sodium soy sa...|\n",
      "| 2698|    italian|Italian parsley l...|\n",
      "|41995|    mexican|ground cinnamon, ...|\n",
      "|31908|    italian|fresh parmesan ch...|\n",
      "|24717|     indian|tumeric, vegetabl...|\n",
      "|34466|    british|greek yogurt, lem...|\n",
      "| 1420|    italian|italian seasoning...|\n",
      "| 2941|       thai|sugar, hot chili,...|\n",
      "| 8152| vietnamese|soy sauce, vegeta...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train2 = train\\\n",
    "    .withColumn(\"ingreds\", \n",
    "                col(\"ingredients\").cast(StringType()))\\\n",
    "    .withColumn(\"ingredientes\",\n",
    "               regexp_replace(col(\"ingreds\"), pattern=\"[\\[\\]]\", replacement=\"\"))\\\n",
    "    .select(\"id\", \"cuisine\", col(\"ingredientes\").alias(\"ingredients\"))\n",
    "train2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de librerías Spark NLP \n",
    "\n",
    "* <https://github.com/JohnSnowLabs/spark-nlp/issues/106>\n",
    "* <https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setup sparknlp source\n",
    "## \n",
    "## https://github.com/JohnSnowLabs/spark-nlp/issues/106\n",
    "## https://stackoverflow.com/questions/34302314/no-module-name-pyspark-error\n",
    "import os, glob, sys\n",
    "sys.path.extend(glob.glob(\"/home/jaa6766/spark-nlp_2.11-1.5.0.jar\"))\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "import os, glob, sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      "\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id   |cuisine    |ingredients                                                                                                                        |\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10259|greek      |romain@lettuc@,@black@oliv@,@grape@tomato@,@garlic@,@pepper@,@purpl@onion@,@season@,@garbanzo@bean@,@feta@chees@crumbl             |\n",
      "|25693|southern_us|plain@flour@,@ground@pepper@,@salt@,@tomato@,@ground@black@pepper@,@thym@,@egg@,@green@tomato@,@yellow@corn@meal@,@milk@,@veget@oil|\n",
      "+-----+-----------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docAssemblr = DocumentAssembler()\\\n",
    "  .setInputCol(\"ingredients\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizr = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")#    .addInfixPattern(\"(\\p{L}+)(n't\\b)\") \\\n",
    "    \n",
    "normalizr = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setPattern(\"[^A-Za-z,]\")\n",
    "    \n",
    "stemmr = Stemmer() \\\n",
    "  .setInputCols([\"normalized\"]) \\\n",
    "  .setOutputCol(\"stems\")\n",
    "    \n",
    "finishr = Finisher() \\\n",
    "    .setInputCols([\"stems\"]) \\\n",
    "    .setOutputCols([\"ingredients\"]) \\\n",
    "    .setIncludeKeys(False)\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "    docAssemblr,\n",
    "    tokenizr, \n",
    "    normalizr,\n",
    "    stemmr,\n",
    "    finishr\n",
    "])\n",
    "\n",
    "train.cache()\n",
    "model = pipeline.fit(train2)\n",
    "train3 = model.transform(train2)\n",
    "train3.printSchema()\n",
    "train3.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canastas\n",
    "\n",
    "En este primer paso lo que realizamos es volver a pasar los datos a array, para que sean canastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cuisine: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-----+-----------+--------------------+\n",
      "|   id|    cuisine|         ingredients|\n",
      "+-----+-----------+--------------------+\n",
      "|10259|      greek|[purpl@onion, bla...|\n",
      "|25693|southern_us|[veget@oil, salt,...|\n",
      "|20130|   filipino|[chicken@liver, g...|\n",
      "|22213|     indian|[water, veget@oil...|\n",
      "|13162|     indian|[water, cayenn@pe...|\n",
      "| 6602|   jamaican|[ground@ginger, f...|\n",
      "|42779|    spanish|[flat@leaf@parsle...|\n",
      "| 3735|    italian|[flour, white@alm...|\n",
      "|16903|    mexican|[iceberg@lettuc, ...|\n",
      "|12734|    italian|[flat@leaf@parsle...|\n",
      "| 5875|    italian|[mushroom, canola...|\n",
      "|45887|    chinese|[crush@red@pepper...|\n",
      "| 2698|    italian|[italian@parslei@...|\n",
      "|41995|    mexican|[avocado, crush@r...|\n",
      "|31908|    italian|[allpurpos@flour,...|\n",
      "|24717|     indian|[spinach, sweet@p...|\n",
      "|34466|    british|[confection@sugar...|\n",
      "| 1420|    italian|[italian@season, ...|\n",
      "| 2941|       thai|[asian@fish@sauc,...|\n",
      "| 8152| vietnamese|[veget@oil, chick...|\n",
      "+-----+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 39.3 ms, sys: 20.8 ms, total: 60.1 ms\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "udf_ingredients = udf(lambda ingredients: \n",
    "                      list(set(ingredients)), \n",
    "                      returnType=ArrayType(StringType()))\n",
    "train4 = train3 \\\n",
    "    .withColumn(\"ingredients\", regexp_replace(\"ingredients\", \"@?,@?\", \",\")) \\\n",
    "    .select(\"id\", \"cuisine\",\n",
    "        split(\"ingredients\", \"\\s*,\\s*\").alias(\"ingredients\")) \\\n",
    "    .cache() \\\n",
    "    .withColumn(\"ingredients\", udf_ingredients(\"ingredients\"))    \n",
    "#.select( \\\n",
    "#        \"id\",\n",
    "#        \"cuisine\",\n",
    "#        regexp_replace(\"ingredients\", \"\\@\", \"@ \").alias(\"ingredients\")\\\n",
    "#    ) \\\n",
    "train4.write.parquet(\"hdfs://172.17.0.3:9000/ma2018-examen2/tmp-train4.parquet\", mode=\"overwrite\")\n",
    "train4 = spark.read.parquet(\"hdfs://172.17.0.3:9000/ma2018-examen2/tmp-train4.parquet\")\n",
    "train4.printSchema()\n",
    "train4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 ms, sys: 5.95 ms, total: 23 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fp = FPGrowth(minSupport=0.1, minConfidence=0.2, itemsCol=\"ingredients\")\n",
    "fpm = fp.fit(train4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|items                |freq |\n",
      "+---------------------+-----+\n",
      "|[salt]               |18048|\n",
      "|[onion]              |7972 |\n",
      "|[oliv@oil]           |7971 |\n",
      "|[water]              |7457 |\n",
      "|[garlic]             |7380 |\n",
      "|[sugar]              |6434 |\n",
      "|[garlic@clove]       |6236 |\n",
      "|[butter]             |4847 |\n",
      "|[ground@black@pepper]|4784 |\n",
      "|[allpurpos@flour]    |4632 |\n",
      "|[pepper]             |4438 |\n",
      "|[onion, salt]        |4392 |\n",
      "|[veget@oil]          |4385 |\n",
      "|[oliv@oil, salt]     |4177 |\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.freqItemsets.orderBy(col(\"freq\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reglas de Asociación y Predicciones\n",
    "\n",
    "Al parecer no se observan reglas de asociación. Por ende, no hay predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+\n",
      "|antecedent|consequent|         confidence|\n",
      "+----------+----------+-------------------+\n",
      "|    [salt]|[oliv@oil]|0.23143838652482268|\n",
      "|    [salt]|   [onion]|0.24335106382978725|\n",
      "|[oliv@oil]|    [salt]| 0.5240245891356166|\n",
      "|   [onion]|    [salt]| 0.5509282488710486|\n",
      "+----------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------------+-----------------+\n",
      "|   id|    cuisine|         ingredients|       prediction|\n",
      "+-----+-----------+--------------------+-----------------+\n",
      "|10259|      greek|[purpl@onion, bla...|               []|\n",
      "|25693|southern_us|[veget@oil, salt,...|[oliv@oil, onion]|\n",
      "|20130|   filipino|[chicken@liver, g...|[oliv@oil, onion]|\n",
      "|22213|     indian|[water, veget@oil...|[oliv@oil, onion]|\n",
      "|13162|     indian|[water, cayenn@pe...|       [oliv@oil]|\n",
      "+-----+-----------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(train4).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplos con ingredientes arbitrarios\n",
    "* **Salt**, Eggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "| ingredients|       prediction|\n",
      "+------------+-----------------+\n",
      "|[salt, eggs]|[oliv@oil, onion]|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"salt\", \"eggs\"], )], [\"ingredients\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['old@el@paso@mild@red@enchilada@sauc',\n",
       " 'cook@chicken',\n",
       " 'mexican@chees@blend',\n",
       " 'pillsburi@refriger@crescent@dinner@roll',\n",
       " 'red@enchilada@sauc',\n",
       " 'refriger@crescent@roll']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "spark.registerDataFrameAsTable(train3, \"train3\")\n",
    "spark.registerDataFrameAsTable(train4, \"train4\")\n",
    "ingredients_nonunique = spark\\\n",
    "    .sql(\"SELECT ingredients FROM train4 WHERE id = 1667 OR \\\n",
    "         array_contains(ingredients, 'old@ el@ paso@ mild@ red@ enchilada@ sauc@ ')\") \\\n",
    "    .collect()[0].ingredients\n",
    "ingredients_nonunique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lettuce, Tomato, **Olive@oil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------+\n",
      "|ingredients                |prediction|\n",
      "+---------------------------+----------+\n",
      "|[lettuce, tomato, oliv@oil]|[salt]    |\n",
      "+---------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.transform(spark.createDataFrame([([\"lettuce\", \"tomato\", \"oliv@oil\"], )], [\"ingredients\"])).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.44 ms, sys: 6.28 ms, total: 10.7 ms\n",
      "Wall time: 2.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6681"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train4.select(explode(\"ingredients\")).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NorvigSweetingModel\n",
    "model = NorvigSweetingModel.pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Missing annotators in pipeline. Make sure the following are present: token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o308.transform.\n: java.lang.IllegalArgumentException: requirement failed: Missing annotators in pipeline. Make sure the following are present: token\n\tat scala.Predef$.require(Predef.scala:224)\n\tat com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a81bbc04d062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/intel/intelpython3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Missing annotators in pipeline. Make sure the following are present: token'"
     ]
    }
   ],
   "source": [
    "model.transform(train4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin del Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
