---
title: "Examen 1 - Ejercicio 3"
output:
  html_document:
    df_print: paged
---

_175904 - Jorge III Altamirano Astorga_

_177508 - Uriel Miranda Miñón_

```{r include=FALSE}
library(tidyverse)
options("mc.cores" = 15L)
library(sparklyr)
```


Datos de [MovieLens](http://movielens.org/)

# {.tabset}

## Carga de datos


```{r}
ratings <- read_csv("movielens/ratings.csv", 
                  col_names = c("userId", "timestamp"),
                  col_types = "i__i", 
                  # col_names = c("userId", "movieId", "rating", "timestamp"),
                  # col_types = "iidi",
                  skip =  1,
                  progress = F) 
tags <- read_csv("movielens/tags.csv", 
                  col_names = c("userId", "timestamp"),
                  col_types = "i--i", 
                  # col_names = c("userId", "movieId", "tag", "timestamp"),
                  # col_types = "iici",
                  skip =  1,
                  progress = F) 
nrow(ratings)
nrow(tags)
ratings <- rbind(ratings, tags) %>% 
  arrange(timestamp) %>%
  select(-timestamp) %>%
  rowid_to_column("id")
rm(tags)
nrow(ratings)
ratings %>% head(n=10)
```

```{r}
summary(ratings)
```

Dividir el set en pruebas y validación.

```{r}
ratings_1 <- ratings[1:1E6,]
ratings_2 <- ratings[(1E6+1):nrow(ratings),]
summary(ratings_1)
summary(ratings_2)
```

## Conteos exactos

Del total

```{r}
n_users <- ratings$userId %>% unique %>% length
n_recor <- nrow(ratings)
sprintf("Se encontraron %d usuarios en %d registros. Proporción: %.3f",
        n_users,
        n_recor,
        n_users/n_recor)
```

Del set de entrenamiento

```{r}
n_users <- ratings_1$userId %>% unique %>% length
n_recor <- nrow(ratings_1)
sprintf("Se encontraron %d usuarios en %d registros. Proporción: %.3f",
        n_users,
        n_recor,
        n_users/n_recor)
```

Del set de prueba

```{r}
n_users <- ratings_2$userId %>% unique %>% length
n_recor <- nrow(ratings_2)
sprintf("Se encontraron %d usuarios en %d registros. Proporción: %.3f",
        n_users,
        n_recor,
        n_users/n_recor)
```

Cambiamos los dataframes para que estén como texto la columna de User ID

```{r}
ratings_1$userId <- as.character(ratings_1$userId)
ratings_2$userId <- as.character(ratings_2$userId)
ratings_1 <- ratings_1[,2]
ratings_2 <- ratings_2[,2]
glimpse(ratings_1)
glimpse(ratings_2)
```


## Conteo Probabilístico

### Buckets

```{r}
set.seed(752)
n <- 100000
m_bits <- 5
m <- 2^m_bits ## aprox 262,144
tail_length_lead <- function(bits){
  bits[-c(1:m_bits)] %>% which.max %>% as.integer
}
cubeta <- function(bits){
  paste(as.character(bits[1:m_bits]), collapse='')
}
armonica <- function(x){
  1/mean(1/x)
}
hash_gen <- function(seed){
  function(x){
    hash_32 <- digest::digest(x, 'xxhash32', serialize = FALSE, seed = seed) 
    # Covertimos a bits, tomando de dos en dos:
    sapply(seq(1, nchar(hash_32), 2), function(x) substr(hash_32, x, x+1)) %>%
      strtoi(16L) %>%  # a enteros
      as.raw %>%    #bytes
      rawToBits()
    #dig_md5[permutacion]
  }
}
hash_1 <- hash_gen(seed = 752)
# n = log2(262144) #máx bits y cubetas
# n
# n_2 <- 5000
# ratings_1hll <- data_frame(num_distintos = 1:n_2) %>%
#       mutate(user_id = as.character(
#         ratings_1$userId[sample.int(nrow(ratings_1), n_2)]
#         )
#       ) %>%
#       mutate(tail = map_int(user_id, function(x) { 
#                      hash_1(x) %>% tail_length_lead }), 
#              cubeta = map_chr(user_id, function(x) { 
#                     hash_1(x) %>% cubeta }),
#              tail_max = cummax(tail))
```

```{r}
ratings_hll <- ratings_1 %>%
      mutate(
        n_diferentes = 1:nrow(ratings_1),
        # n_diferentes = 1:100,map_chr(buckt_tail, function(x) x[2])
        buckt_tail = map(userId, function(x) { 
          hash_x = hash_1(x)
          c(
            bucket = cubeta(hash_x),
            tail = tail_length_lead(hash_x)
          )
        }),
        bucket = map_chr(buckt_tail, function(x) x[1]),
        tail = map_chr(buckt_tail, function(x) x[2]))
ratings_hll <- ratings_hll[,c(1,2,4,5)]
ratings_hll$tail <- as.integer(ratings_hll$tail)
ratings_hll
```

Número de usuarios por conteo probabilístico:

```{r}
ratings_hll$userId %>% unique %>% length
```

## Gráficas

```{r}
graph <- ratings_hll[sample.int(nrow(ratings_hll), (nrow(ratings_hll)*0.1)),] %>%
  spread(bucket, tail, fill= 0) %>%
  gather(bucket, tail, -n_diferentes, -userId) %>%
  select(n_diferentes, bucket, tail)
graph <- graph %>% 
  group_by(bucket) %>%
  arrange(n_diferentes) %>%
  mutate(tail_max = cummax(tail)) %>%
  group_by(n_diferentes) %>%
  summarise(mean = 0.72*(m*armonica(2^tail_max)))
ggplot(graph %>% filter(n_diferentes > 1000), aes(x = n_diferentes, y = mean)) + 
  geom_line() +
  geom_abline(intercept = 0, slope = 0.018, colour ='red')
```

```{r}
quantile(1-graph$mean/graph$n_diferentes, probs=c(0.1, 0.5, .9))
```

Error relativo 

```{r}
1.04/sqrt(ratings_hll$bucket %>% unique %>% length)
```


## Examen


1. Utiliza el algoritmo de hyperloglog para contar el número de
usuarios distintos en los datos del ejercicio anterior, considerando que el flujo de datos son los ratings (ordénalos primero por timestamp). 
Utiliza el primer millón de observaciones.
Haz una gráfica donde el eje horizontal es el número de elementos del flujo vistos y el eje
vertical es la estimación de hyperloglog de usuarios únicos para cada momento.

_Se realizó en la sección de "Conteo Probabilístico"._

2. Compara la estimación final de hyperloglog con el número
de usuarios únicos (primer millón de observaciones ordenadas
por timestamp).

_Nuestro conteo exacto fue de 138493 y el probabilístico fue 16710. Incluso utilizando pyspark coincidió. ESto es por la distribución de la muestra._

_Aunque con Sparklyr no lo pudimos realizar, pyspark funcionó de maravilla: ¡menos de 1 minuto!_

_Aquí están nuestros resultados: [Jupyter Notebook](https://philwebsurfer.github.io/metodos-analiticos-2018/alumnos/jorge_altamirano/metodos-analiticos-jaa/examen1/examen1_3.pyspark.html)._